{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to find the most suitable K value for the KNN filling algorithm\n",
    "Since Logistic regression performs and worst among all algorithm, we don't use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask_ml\n",
    "import joblib\n",
    "\n",
    "import dask\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.csvcsv)\n",
    "import sklearn as sk\n",
    "# import statsmodels.api as sm\n",
    "# # Cross Validation Classification Accuracy\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold,cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing \n",
    "# from sklearn.impute import KNNImputer\n",
    "# #import matplotlib.pyplot as plot\n",
    "# from matplotlib import pyplot\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn import tree\n",
    "# import statistics\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# plotting errorbar graph for visual representation of the performance of the classifiers\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.transforms import Affine2D\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "# import shutil\n",
    "# import os\n",
    "from our_evaluate_sepsis_score import evaluate_performance_dask\n",
    "import our_functions_library as flib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this once\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this when the client has started\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataType={'Age': 'int', 'Gender': bool, 'Patient_id': int, 'time': 'int8', 'SepsisLabel':'float16'}\n",
    "filename = '../../raw_data/raw_data_400.csv' # use raw dataset\n",
    "originalData = dd.read_csv(filename,dtype={'Gender':bool,'SepsisLabel':bool}) # read csv data into Dask DataFrame\n",
    "Uniq_ID= np.unique(originalData['Patient_id'])\n",
    "print(\"Data size:\",originalData.shape)\n",
    "print('Patient id size:',len(Uniq_ID))\n",
    "print(Uniq_ID)\n",
    "\n",
    "X_columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2','BaseExcess', 'HCO3', 'FiO2', 'pH', \n",
    "             'PaCO2', 'SaO2', 'AST', 'BUN','Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium','Bilirubin_total', 'TroponinI', 'Hct', \n",
    "             'Hgb', 'PTT', 'WBC','Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2','HospAdmTime',\n",
    "             'ICULOS', 'Patient_id', 'time']\n",
    "y_columns = ['Patient_id', 'SepsisLabel']\n",
    "\n",
    "# To test if these 3 columns is useful\n",
    "# originalData = originalData.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "# X_columns.remove('Unit1')\n",
    "# X_columns.remove( 'Unit2')\n",
    "# X_columns.remove( 'EtCO2')\n",
    "\n",
    "# Initialize the empty array X_train, X_test, y_train, y_test\n",
    "# X_train = dd.from_pandas(pd.DataFrame(columns = X_columns),npartitions=2)\n",
    "# X_test = dd.from_pandas(pd.DataFrame(columns = X_columns),npartitions=2)\n",
    "# y_train = dd.from_pandas(pd.DataFrame(columns = y_columns),npartitions=2)\n",
    "# y_test = dd.from_pandas(pd.DataFrame(columns = y_columns),npartitions=2)\n",
    "\n",
    "# Below are the lists for KNN results of different K value\n",
    "KNN_UtilityScore_mean = []\n",
    "KNN_UtilityScore_std = []\n",
    "KNN_F1Score_mean = []\n",
    "KNN_F1Score_std = []\n",
    "KNN_auroc_mean = []\n",
    "KNN_auprc_mean = []\n",
    "KNN_accuracy_mean = []\n",
    "KNN_accuracy_std = []\n",
    "KNN_positiveprediction_mean = []\n",
    "KNN_baseline_mean = [ ]\n",
    "KNN_total_Time= [ ]\n",
    "fillmethod =\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(15348, 43)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalData.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 patient ids are the same. Uniq id hasn't been shuffled, and it will be shuffeld now.\n",
      "The original uniq id set is:\n",
      " [211 275 153 189 184 110 124  49 345  73 284  20  71 328 136 241 171 141\n",
      " 329 204]\n",
      "The fixed shuffelld id set, it should be 211 275 153 189 184 110 124  49...\n",
      " [211 275 153 189 184 110 124  49 345  73 284  20  71 328 136 241 171 141\n",
      " 329 204]\n"
     ]
    }
   ],
   "source": [
    "#This block will shuffle the id sets of patients in a fix manner, so for every time you run the code, the dataset and trainset are always the same.\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "if(np.array_equal(range(1,21),Uniq_ID[:20]) ):\n",
    "    print(\"The first 20 patient ids are the same. Uniq id hasn't been shuffled, and it will be shuffeld now.\")   \n",
    "    random.shuffle(Uniq_ID)# randomly sorted the patient IDs \n",
    "#     Uniq_ID = da.from_array(Uniq_ID)\n",
    "else:\n",
    "    print(\"They are not the same. Uniq_id has already been shuffled.\")    \n",
    "print(\"The original uniq id set is:\\n\",Uniq_ID[0:20])\n",
    "print(\"The fixed shuffelld id set, it should be 211 275 153 189 184 110 124  49...\\n\",Uniq_ID[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_reset():\n",
    "    KNN_UtilityScore_mean.clear()\n",
    "    KNN_UtilityScore_std.clear()\n",
    "    KNN_F1Score_mean.clear()\n",
    "    KNN_F1Score_std.clear()\n",
    "    KNN_auroc_mean.clear()\n",
    "    KNN_auprc_mean.clear()\n",
    "    KNN_accuracy_mean.clear()\n",
    "    KNN_accuracy_std.clear()\n",
    "    KNN_positiveprediction_mean.clear()\n",
    "    KNN_baseline_mean.clear()\n",
    "    KNN_total_Time.clear()\n",
    "    \n",
    "\n",
    "def generateTrainDataSet(test_patienIds):\n",
    "    train_data= originalData[~(originalData.Patient_id.isin(test_patienIds))]\n",
    "    X_train = train_data[X_columns]\n",
    "    y_train = train_data[y_columns] \n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def generateTestDataSet(patienIds):\n",
    "    print(\"test_patienIds: \\n\", patienIds)\n",
    "    test_data = originalData[originalData['Patient_id'].isin(patienIds)]\n",
    "    X_test = test_data[X_columns]\n",
    "    y_test = test_data[y_columns]\n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "def KNNfilling(trainData,testData,K= 5):\n",
    "    imputer = KNNImputer(n_neighbors = K)\n",
    "    #imputer = FaissKNeighbors(k=K)\n",
    "    imputer.fit(trainData)\n",
    "    x_train_impute=imputer.transform(trainData).round(3).astype('float32')\n",
    "    x_test_impute=imputer.transform(testData).round(3).astype('float32')\n",
    "#     x_train_impute=imputer.transform(trainData)\n",
    "#     x_test_impute=imputer.transform(testData)\n",
    "    fillmethod= \"KnnFill\"\n",
    "    return x_train_impute, x_test_impute,fillmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for 1 fold in Kfold\n",
    "def OneFold_patient_MP(i,model,KforKNN,idSets,fillmethod): \n",
    "    start1 = time.time()\n",
    "    print(\"for the\",i+1,\"th iteration: \")\n",
    "    X_test,y_test = generateTestDataSet(idSets[i])\n",
    "    print(\"Time for splitting id to test dataset:\", round(time.time()- start1,3))\n",
    "    t1 = time.time()\n",
    "    X_train, y_train = generateTrainDataSet(idSets[i])\n",
    "    print(\"Time for splitting id to train dataset:\", round(time.time()- t1,3))\n",
    "\n",
    "    #Now the train and test dataset is generated \n",
    "    patientID_ytest = y_test['Patient_id']\n",
    "    y_test = y_test.drop('Patient_id', 1) \n",
    "    y_train = y_train.drop('Patient_id', 1)\n",
    "#     X_train=X_train.astype('float16')\n",
    "#     X_test=X_test.astype('float16') \n",
    "    X_train=X_train.astype('float32')\n",
    "    X_test=X_test.astype('float32') \n",
    "    y_train=y_train.astype('bool')\n",
    "    y_test=y_test.astype('bool')\n",
    "    print(X_train)\n",
    "    \n",
    "    #fill the missing data\n",
    "    if X_train.isnull().values.any() or X_test.isnull().values.any() :\n",
    "        t1= time.time()\n",
    "        print(\"X_train or X_test contains NaN values, KNN is performed.\")\n",
    "#         X_train, X_test,fillmethod = flib.MeanFilling(X_train,X_test, fillmethod)  \n",
    "        X_train, X_test, fillmethod = flib.KNNfilling(X_train, X_test, KforKNN)\n",
    "        print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "        print(\"Time for NaN values filling: \", round(time.time()- t1,2) )\n",
    "        X_train = dd.from_array(X_train)\n",
    "        X_test = dd.from_array(X_test)\n",
    "#         if np.isnan(X_train).any() or np.isnan(X_test).any() :\n",
    "#             print(\"X_train_impute or X_test_impute still contains NaN values\")          \n",
    "    print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "    \n",
    "    \n",
    "    if ( type(model).__name__ == 'LogisticRegression'):  #Scale the data for logistic regression\n",
    "        t1 = time.time()\n",
    "        scaler = dask_ml.preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        print(\"Standard Scaling time: \", round(time.time()- t1,2) )\n",
    "\n",
    "               \n",
    "    #fit the model and predict\n",
    "    with joblib.parallel_backend(\"dask\",scatter=[X_train, y_train]):\n",
    "        model.fit(X_train, y_train)\n",
    "        # y_predicted = dd.from_array(model.predict(X_test).astype(bool))\n",
    "        y_predicted_probobility = dd.from_array(model.predict_proba(X_test).astype('float16').round(4),columns='prob')\n",
    "    # y_labels = y_test['SepsisLabel'].reset_index(drop=True)\n",
    "    del X_train, X_test, y_train\n",
    "\n",
    "    print(\"columns of y_predicted_probobility is: \",len(y_predicted_probobility[0]))\n",
    "    if(len(y_predicted_probobility[0]) ==2 ):\n",
    "        y_predicted_probobility =  y_predicted_probobility[:,1].round(4)\n",
    "\n",
    "    \n",
    "    # auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance_dask(y_labels, y_predicted, y_predicted_probobility,patientID_ytest,idSets[i])\n",
    "    # del y_labels, y_predicted_probobility\n",
    "    # result = { \"auroc\": round(auroc,4), \"auprc\": round(auprc,4), \"f_measure\": round(f_measure,4),\n",
    "    #             \"physio_accuracy\": round(physio_accuracy,4),\"utility_score\": round(utility_score,4),\n",
    "    #             \"positiveprediction\": np.sum(y_predicted),\n",
    "    #         \"baseline\": round( (1 - y_test.mean().compute() )*100 , 2 )}\n",
    "    # print(\"\\ny_test size:\",y_test.shape, '1´s in y_test',y_test.sum())\n",
    "    del  y_test\n",
    "    print(\"Time spent in \",i,\"th Fold iteration\",round(time.time()-start1,2),\"sec.\\n\")\n",
    "    print(\"******************************************************************\")\n",
    "    # return result\n",
    "    return y_predicted_probobility.compute().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predicted_probobility' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-101-fa5919680eb2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0my_predicted_probobility\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_predicted_probobility' is not defined"
     ]
    }
   ],
   "source": [
    "y_predicted_probobility\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "\n",
    "#%t\n",
    "\n",
    "# Code for K-fold algorithm using multi processes\n",
    "def KFold_patient_DASKMP(model, KforKFold=10,KforKNN=5, fillmethod=\"\"):\n",
    "    start = time.time()# time indicator for how long the Kfold func takes  \n",
    "#     global X_train, X_test, y_train, y_test\n",
    "    print(\"Datasize\",len(Uniq_ID) ,\"with K Fold of \",KforKFold ,\" folds filling by KNN =\",KforKNN)\n",
    "    # initialisation of the array for storing the different intermediate results\n",
    "    auroc_model = []\n",
    "    auprc_model = []\n",
    "    f_measure_model = []\n",
    "    physio_accuracy_model = []\n",
    "    utility_score_model = []\n",
    "    positivepredictions = []\n",
    "    baseline_model= []\n",
    "    #The unique id sets have been created and shuffled in a fix manner in the third block, you can just use it here and no more any other manipulation\n",
    "    idSets = np.array_split(Uniq_ID, KforKFold)# divide the ids into K groups\n",
    "\n",
    "    #write the idset data into a file\n",
    "    open(\"test.txt\", \"w\").close() # clear contents of existing file\n",
    "    for i in range(KforKFold):\n",
    "        splitted = \" \".join( repr(e) for e in idSets[i])\n",
    "        file1 = open(\"test.txt\",\"a\")\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.write(str(\"[\" +splitted+\"]\"))\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.close()\n",
    "    print(\"Time for writing id to file:\", round(time.time()- start,3))\n",
    "    # OneFold_patient_MP(1, model, KforKNN,idSets,fillmethod)\n",
    "    futures = []\n",
    "    for i in range(0,10):\n",
    "        print(i)\n",
    "        f = client.submit(OneFold_patient_MP,i, model, KforKNN,idSets,fillmethod)\n",
    "        futures.append(f)\n",
    "    y_labels = originalData['SepsisLabel']\n",
    "    patientID_ytest = originalData['Patient_id']\n",
    "    y_predicted_probobility= dd.from_pandas(pd.DataFrame(),npartitions=2)\n",
    "\n",
    "    for f in futures:\n",
    "        print(f.result())\n",
    "        print(f.result().shape)\n",
    "        print(type(f.result()))\n",
    "        y_predicted_probobility = y_predicted_probobility.append(f.result())\n",
    "        print(\"*********************************\")\n",
    "    print(y_predicted_probobility.compute())\n",
    "    y_predicted = (y_predicted_probobility.compute() >= 0.5).to_numpy()\n",
    "    print(y_predicted)\n",
    "    auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance_dask( y_labels.compute(), y_predicted, y_predicted_probobility.compute(),patientID_ytest.compute(),idSets)\n",
    "    print('utility F1 of model:', f_measure)\n",
    "    print('Utility accuracy of model:', physio_accuracy)\n",
    "    print('Utility score of model:', utility_score)\n",
    "       # result = f.result()\n",
    "        # auroc_model.append(result[\"auroc\"])\n",
    "        # auprc_model.append(result[\"auprc\"])\n",
    "        # f_measure_model.append(result[\"f_measure\"])\n",
    "        # physio_accuracy_model.append(result[\"physio_accuracy\"])\n",
    "        # utility_score_model.append(result[\"utility_score\"])\n",
    "        # positivepredictions.append(result[\"positiveprediction\"])\n",
    "        # baseline_model.append(result[\"baseline\"])\n",
    "    # print(\"\\nEvaluation parameters of the utiltiy evaluation function:\")\n",
    "    # print('auroc of model:', auroc_model)\n",
    "    # print('auprc of model:', auprc_model)\n",
    "    # print('utility F1 of model:', f_measure_model)\n",
    "    # print('Utility accuracy of model:', physio_accuracy_model)\n",
    "    # print('Utility score of model:', utility_score_model)\n",
    "    # print(\"Positive Prediction: \", positivepredictions)\n",
    "    # print('Baseline model:', baseline_model)\n",
    "    # KNN_auroc_mean.append(np.mean(auroc_model))\n",
    "    # KNN_auprc_mean.append(np.mean(auprc_model))\n",
    "    # KNN_F1Score_mean.append(np.mean(f_measure_model))\n",
    "    # KNN_F1Score_std.append(np.std(f_measure_model))\n",
    "    # KNN_accuracy_mean.append(np.mean(physio_accuracy_model))\n",
    "    # KNN_accuracy_std.append(np.std(physio_accuracy_model))\n",
    "    # KNN_UtilityScore_mean.append(np.mean(utility_score_model))\n",
    "    # KNN_UtilityScore_std.append(np.std(utility_score_model))\n",
    "    # KNN_positiveprediction_mean.append(np.mean(positivepredictions))\n",
    "    # KNN_baseline_mean.append(np.mean(baseline_model))\n",
    "    #\n",
    "    # totalTime=round(time.time()- start,2)\n",
    "    # KNN_total_Time.append(totalTime)\n",
    "    # print(\"\\nTotal Time spent in  KFold function\",totalTime,\"sec.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                Probol\n",
      "npartitions=2         \n",
      "0              float64\n",
      "3                  ...\n",
      "4                  ...\n",
      "Dask Name: from_pandas, 2 tasks\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "   Probol\n",
      "0     1.0\n",
      "1     0.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4     0.0\n",
      "0     0.8\n",
      "1     0.3\n",
      "2     0.5\n",
      "3     0.9\n",
      "4     0.2\n",
      "0     0.8\n",
      "1     0.3\n",
      "2     0.5\n",
      "3     0.9\n",
      "4     0.2\n"
     ]
    }
   ],
   "source": [
    "df = dd.from_pandas(pd.DataFrame([0.8,0.3,0.5,0.9,0.2],columns=[\"Probol\"]),npartitions=2)\n",
    "print(df)\n",
    "df1 = df >= 0.5\n",
    "print(type(df1.compute()))\n",
    "df1 = df1.append(df)\n",
    "df1 = df1.append(df)\n",
    "print()\n",
    "print(df1.compute())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasize 400 with K Fold of  10  folds filling by KNN = 10\n",
      "Time for writing id to file: 0.004\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of column names must match width of the array. Got 4 names for 2 columns",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-125-1dbaa6835faf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# %%time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mKFold_patient_DASKMP\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdecisionTreeModel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-124-83a8f94509d4>\u001B[0m in \u001B[0;36mKFold_patient_DASKMP\u001B[1;34m(model, KforKFold, KforKNN, fillmethod)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfutures\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\distributed\\client.py\u001B[0m in \u001B[0;36mresult\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"error\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m             \u001B[0mtyp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"cancelled\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-123-d8544209bbb1>\u001B[0m in \u001B[0;36mOneFold_patient_MP\u001B[1;34m()\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# y_predicted = dd.from_array(model.predict(X_test).astype(bool))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0my_predicted_probobility\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'float16'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mround\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'prob'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m     \u001B[1;31m# y_labels = y_test['SepsisLabel'].reset_index(drop=True)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[1;32mdel\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\io.py\u001B[0m in \u001B[0;36mfrom_array\u001B[1;34m()\u001B[0m\n\u001B[0;32m    115\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mfrom_dask_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeta\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmeta\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m     \u001B[0mmeta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_meta_from_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeta\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmeta\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m     \u001B[0mdivisions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchunksize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\io.py\u001B[0m in \u001B[0;36m_meta_from_array\u001B[1;34m()\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[0mcolumns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             raise ValueError(\n\u001B[0m\u001B[0;32m     71\u001B[0m                 \u001B[1;34m\"Number of column names must match width of the \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m                 \u001B[1;34m\"array. Got {0} names for {1} \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Number of column names must match width of the array. Got 4 names for 2 columns"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "KFold_patient_DASKMP(decisionTreeModel,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(type(y_labels.compute()))\n",
    "# print(y_labels)\n",
    "# y_labels.compute().rename(\"labels\")\n",
    "y_test.compute().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(getsizeof(X_train))\n",
    "print(getsizeof(X_test))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# X_test.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"X_train size is: \",X_train.size * X_train.itemsize)\n",
    "# print(\"X_test size is: \",X_test.size * X_test.itemsize)\n",
    "print(\"X_train size is: \",X_train.memory_usage(deep=True).sum().compute())\n",
    "print(\"X_test size is: \",X_test.memory_usage(deep=True).sum().compute())\n",
    "print(\"y_train size is: \",y_train.memory_usage(deep=True).sum().compute())\n",
    "print(\"y_test size is: \",y_test.memory_usage(deep=True).sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(y_labels)\n",
    "print(y_labels)\n",
    "print(y_predicted)\n",
    "print(type(y_predicted))\n",
    "print(y_predicted_probobility)\n",
    "print(type(y_predicted_probobility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.loc[0:0].compute()\n",
    "# X_train[0:0].compute()\n",
    "# del y_train,y_test,X_test\n",
    "# del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_DATA():\n",
    "    print(\"Numbers of K:\",len(KNN_accuracy_mean))\n",
    "    print(\"KNN_UtilityScore_mean\", KNN_UtilityScore_mean)\n",
    "    print(\"KNN_UtilityScore_std\", KNN_UtilityScore_std)\n",
    "    print(\"KNN_F1Score_mean\", KNN_F1Score_mean)\n",
    "    print(\"KNN_F1Score_std\", KNN_F1Score_std)\n",
    "    print(\"KNN_auroc_mean\", KNN_auroc_mean)\n",
    "    print(\"KNN_auprc_mean\", KNN_auprc_mean)\n",
    "    print(\"KNN_accuracy_mean\", KNN_accuracy_mean)\n",
    "    print(\"KNN_accuracy_std\", KNN_accuracy_std)\n",
    "    print(\"KNN_positiveprediction_mean\", KNN_positiveprediction_mean)\n",
    "    print(\"KNN_baseline_mean\", KNN_baseline_mean)\n",
    "    print(\"KNN_total_time\", KNN_total_Time)      \n",
    "\n",
    "# def findBestKforKNN(model, KforKFold=10,KforKNNstart=1,KforKNNend=10, stepsize=5):\n",
    "#     if(KforKNNend<=1):\n",
    "#         print(\"K must be a interger larger than 1\")\n",
    "#         return\n",
    "#     KNN_reset()\n",
    "#     step=stepsize\n",
    "#     for i in range(KforKNNstart,KforKNNend+1,step) : \n",
    "#         print(\"KNN of K = \",i)    \n",
    "#         KFold_patient(model,10,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='saga', max_iter=1000,penalty='l1')\n",
    "logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='lbfgs', max_iter=1000)\n",
    "decisionTreeModel = tree.DecisionTreeClassifier(random_state=2)\n",
    "randomForestModel = RandomForestClassifier(random_state=2)\n",
    "XGBoostModel = XGBClassifier(random_state=2, n_jobs=4)\n",
    "#AdaBoostModel = AdaBoostClassifier(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing data with one function\n",
    "print(filename)\n",
    "#print(originalData)\n",
    "if (originalData.isnull().values.any()):\n",
    "    print('Data isn´t all filled before K-Fold Func')\n",
    "#data,fillmethod = flib.linearFillingAll(originalData,1, True)#forwardfilling + fill other missing data with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KFold_patient(decisionTreeModel,10,5)\n",
    "Print_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghjhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "j\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_performance_dask(labels,predictions,probabilities,patientids,idSet):\n",
    "    # print(\"****************************************************************************\")\n",
    "    # print(type(labels))\n",
    "    # print(labels)\n",
    "    # print(\"*****************************\")\n",
    "    # print(type(predictions))\n",
    "    # print(predictions)\n",
    "    # print(\"*****************************\")\n",
    "    # print(type(patientids))\n",
    "    # print(patientids)\n",
    "    # print(\"*****************************\")\n",
    "    # print(type(probabilities))\n",
    "    # print(probabilities)\n",
    "    # print(\"*****************************\")\n",
    "    if ( not( len(labels) == len(predictions) == len(probabilities) )):\n",
    "        print(\"The predicted data is not the same in size! Evaluation function will stop here!\" )\n",
    "        return\n",
    "    # Set parameters.\n",
    "    label_header       = 'SepsisLabel'\n",
    "    prediction_header  = 'PredictedLabel'\n",
    "    probability_header = 'PredictedProbability'\n",
    "    dt_early   = -12\n",
    "    dt_optimal = -6\n",
    "    dt_late    = 3\n",
    "    max_u_tp = 1\n",
    "    min_u_fn = -2\n",
    "    u_fp     = -0.05\n",
    "    u_tn     = 0\n",
    "    \n",
    "    auroc, auprc        = compute_auc(labels, probabilities)\n",
    "    accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "    \n",
    "    patientids = patientids.astype('int')\n",
    "    patientdata = patientids.reset_index()\n",
    "#     print(patientids)\n",
    "#     patientdata = patientdata.join(pd.Series(labels.flatten(),name='labels'))\n",
    "    patientdata = patientdata.join(labels.rename('labels'))\n",
    "    patientdata = patientdata.join(pd.Series(predictions.flatten(),name='predictions'))\n",
    "    patientdata = patientdata.drop(\"index\",1)\n",
    "\n",
    "    # print(\"After the joining:\")\n",
    "    # print(patientdata.columns)\n",
    "    # print(patientdata)\n",
    "\n",
    "#     uniq_ids = np.unique(patientdata['Patient_id']) \n",
    "    num_patients = len(idSet)\n",
    "    dataByPatient = patientdata.groupby('Patient_id')\n",
    "\n",
    "    #print(uniq_ids)\n",
    "    \n",
    "    # Compute utility.\n",
    "    observed_utilities = np.zeros(num_patients)\n",
    "    best_utilities     = np.zeros(num_patients)\n",
    "    worst_utilities    = np.zeros(num_patients)\n",
    "    inaction_utilities = np.zeros(num_patients)\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        k = idSet[i]\n",
    "        # print(\" for the patient of id:\",k)\n",
    "        # print(dataByPatient.get_group(k))\n",
    "\n",
    "        labels = dataByPatient.get_group(k).loc[:, 'labels'].to_numpy()\n",
    "        num_rows          = len(labels)\n",
    "        observed_predictions = dataByPatient.get_group(k).loc[:, 'predictions'].to_numpy()\n",
    "\n",
    "        #print(\"labels\",labels)\n",
    "        #print(\"observered_prediction\",observed_predictions)\n",
    "\n",
    "        best_predictions     = np.zeros(num_rows)\n",
    "        worst_predictions    = np.zeros(num_rows)\n",
    "        inaction_predictions = np.zeros(num_rows)\n",
    "\n",
    "        if np.any(labels):\n",
    "            t_sepsis = np.argmax(labels) - dt_optimal\n",
    "            best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, num_rows)] = 1\n",
    "        worst_predictions = 1 - best_predictions\n",
    "\n",
    "        observed_utilities[i] = compute_prediction_utility(labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        best_utilities[i]     = compute_prediction_utility(labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        worst_utilities[i]    = compute_prediction_utility(labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        inaction_utilities[i] = compute_prediction_utility(labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "    unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "    unnormalized_best_utility     = np.sum(best_utilities)\n",
    "    unnormalized_worst_utility    = np.sum(worst_utilities)\n",
    "    unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "    normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "    return auroc, auprc, accuracy, f_measure, normalized_observed_utility\n",
    "\n",
    "\n",
    "# The load_column function loads a column from a table.\n",
    "#\n",
    "# Inputs:\n",
    "#   'filename' is a string containing a filename.\n",
    "#\n",
    "#   'header' is a string containing a header.\n",
    "#\n",
    "# Outputs:\n",
    "#   'column' is a vector containing a column from the file with the given\n",
    "#   header.\n",
    "#\n",
    "# Example:\n",
    "#   Omitted.\n",
    "\n",
    "def load_column(filename: object, header: object, delimiter: object) -> object:\n",
    "    column = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            arrs = l.strip().split(delimiter)\n",
    "            if i == 0:\n",
    "                try:\n",
    "                    j = arrs.index(header)\n",
    "                except:\n",
    "                    raise Exception('{} must contain column with header {} containing numerical entries.'.format(filename, header))\n",
    "            else:\n",
    "                if len(arrs[j]):\n",
    "                    column.append(float(arrs[j]))\n",
    "    return np.array(column)\n",
    "\n",
    "\"\"\n",
    "# The compute_auc function computes AUROC and AUPRC as well as other summary\n",
    "# statistics (TP, FP, FN, TN, TPR, TNR, PPV, NPV, etc.) that can be exposed\n",
    "# from this function.\n",
    "#\n",
    "# Inputs:\n",
    "#   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "#   septic at time i.\n",
    "#\n",
    "#   'predictions' is a probability vector, where predictions[i] gives the\n",
    "#   predicted probability that the patient is septic at time i.  Note that there\n",
    "#   must be a prediction for every label, i.e, len(labels) ==\n",
    "#   len(predictions).\n",
    "#\n",
    "# Outputs:\n",
    "#   'auroc' is a scalar that gives the AUROC of the algorithm using its\n",
    "#   predicted probabilities, where specificity is interpolated for intermediate\n",
    "#   sensitivity values.\n",
    "#\n",
    "#   'auprc' is a scalar that gives the AUPRC of the algorithm using its\n",
    "#   predicted probabilities, where precision is a piecewise constant function of\n",
    "#   recall.\n",
    "#\n",
    "# Example:\n",
    "#   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "#   In [2]: predictions = [0.3, 0.4, 0.6, 0.7, 0.8, 0.8]\n",
    "#   In [3]: auroc, auprc = compute_auc(labels, predictions)\n",
    "#   In [4]: auroc\n",
    "#   Out[4]: 1.0\n",
    "#   In [5]: auprc\n",
    "#   Out[5]: 1.0\n",
    "\n",
    "def compute_auc(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    #print('labels inside auc_fucn',labels)\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not 0 <= prediction <= 1:\n",
    "                warnings.warn('Predictions do not satisfy 0 <= prediction <= 1.')\n",
    "\n",
    "    # Find prediction thresholds.\n",
    "    thresholds = np.unique(predictions)[::-1]\n",
    "    if thresholds[0] != 1:\n",
    "        thresholds = np.insert(thresholds, 0, 1)\n",
    "    if thresholds[-1] == 0:\n",
    "        thresholds = thresholds[:-1]\n",
    "\n",
    "    n = len(labels)\n",
    "    m = len(thresholds)\n",
    "\n",
    "    # Populate contingency table across prediction thresholds.\n",
    "    tp = np.zeros(m)\n",
    "    fp = np.zeros(m)\n",
    "    fn = np.zeros(m)\n",
    "    tn = np.zeros(m)\n",
    "\n",
    "    # Find indices that sort the predicted probabilities from largest to\n",
    "    # smallest.\n",
    "    idx = np.argsort(predictions)[::-1]\n",
    "\n",
    "    i = 0\n",
    "    for j in range(m):\n",
    "        # Initialize contingency table for j-th prediction threshold.\n",
    "        if j == 0:\n",
    "            tp[j] = 0\n",
    "            fp[j] = 0\n",
    "            fn[j] = np.sum(labels)\n",
    "            tn[j] = n - fn[j]\n",
    "        else:\n",
    "            tp[j] = tp[j - 1]\n",
    "            fp[j] = fp[j - 1]\n",
    "            fn[j] = fn[j - 1]\n",
    "            tn[j] = tn[j - 1]\n",
    "\n",
    "        # Update contingency table for i-th largest predicted probability.\n",
    "        while i < n and predictions[idx[i]] >= thresholds[j]:\n",
    "            if labels[idx[i]]:\n",
    "                tp[j] += 1\n",
    "                fn[j] -= 1\n",
    "            else:\n",
    "                fp[j] += 1\n",
    "                tn[j] -= 1\n",
    "            i += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    tpr = np.zeros(m)\n",
    "    tnr = np.zeros(m)\n",
    "    ppv = np.zeros(m)\n",
    "    npv = np.zeros(m)\n",
    "\n",
    "    for j in range(m):\n",
    "        if tp[j] + fn[j]:\n",
    "            tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "        else:\n",
    "            tpr[j] = 1\n",
    "        if fp[j] + tn[j]:\n",
    "            tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "        else:\n",
    "            tnr[j] = 1\n",
    "        if tp[j] + fp[j]:\n",
    "            ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "        else:\n",
    "            ppv[j] = 1\n",
    "        if fn[j] + tn[j]:\n",
    "            npv[j] = tn[j] / (fn[j] + tn[j])\n",
    "        else:\n",
    "            npv[j] = 1\n",
    "\n",
    "    # Compute AUROC as the area under a piecewise linear function with TPR /\n",
    "    # sensitivity (x-axis) and TNR / specificity (y-axis) and AUPRC as the area\n",
    "    # under a piecewise constant with TPR / recall (x-axis) and PPV / precision\n",
    "    # (y-axis).\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    for j in range(m-1):\n",
    "        auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])\n",
    "        auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]\n",
    "\n",
    "    return auroc, auprc\n",
    "\n",
    "# The compute_accuracy_f_measure function computes the accuracy and F-measure\n",
    "# for a patient.\n",
    "#\n",
    "# Inputs:\n",
    "#   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "#   septic at time i.\n",
    "#\n",
    "#   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "#   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "#   patient is predicted to be septic at time i.  Note that there must be a\n",
    "#   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "#\n",
    "# Output:\n",
    "#   'accuracy' is a scalar that gives the accuracy of the predictions using its\n",
    "#   binarized predictions.\n",
    "#\n",
    "#   'f_measure' is a scalar that gives the F-measure of the predictions using its\n",
    "#   binarized predictions.\n",
    "#\n",
    "# Example:\n",
    "#   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "#   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "#   In [3]: accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "#   In [4]: accuracy\n",
    "#   Out[4]: 0.666666666667\n",
    "#   In [5]: f_measure\n",
    "#   Out[5]: 0.666666666667\n",
    "\n",
    "def compute_accuracy_f_measure(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "    # Populate contingency table.\n",
    "    n = len(labels)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] and predictions[i]:\n",
    "            tp += 1\n",
    "        elif not labels[i] and predictions[i]:\n",
    "            fp += 1\n",
    "        elif labels[i] and not predictions[i]:\n",
    "            fn += 1\n",
    "        elif not labels[i] and not predictions[i]:\n",
    "            tn += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    if tp + fp + fn + tn:\n",
    "        accuracy = float(tp + tn) / float(tp + fp + fn + tn)\n",
    "    else:\n",
    "        accuracy = 1.0\n",
    "\n",
    "    if 2 * tp + fp + fn:\n",
    "        f_measure = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "    else:\n",
    "        f_measure = 1.0\n",
    "\n",
    "    return accuracy, f_measure\n",
    "\n",
    "# The compute_prediction_utility function computes the total time-dependent\n",
    "# utility for a patient.\n",
    "#\n",
    "# Inputs:\n",
    "#   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "#   septic at time i.\n",
    "#\n",
    "#   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "#   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "#   patient is predicted to be septic at time i.  Note that there must be a\n",
    "#   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "#\n",
    "# Output:\n",
    "#   'utility' is a scalar that gives the total time-dependent utility of the\n",
    "#   algorithm using its binarized predictions.\n",
    "#\n",
    "# Example:\n",
    "#   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "#   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "#   In [3]: utility = compute_prediction_utility(labels, predictions)\n",
    "#   In [4]: utility\n",
    "#   Out[4]: 3.388888888888889\n",
    "\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "        if dt_early >= dt_optimal:\n",
    "            raise Exception('The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "        if dt_optimal >= dt_late:\n",
    "            raise Exception('The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "    # Does the patient eventually have sepsis?\n",
    "    if np.any(labels):\n",
    "        is_septic = True\n",
    "        t_sepsis = np.argmax(labels) - dt_optimal\n",
    "    else:\n",
    "        is_septic = False\n",
    "        t_sepsis = float('inf')\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    # Define slopes and intercept points for utility functions of the form\n",
    "    # u = m * t + b.\n",
    "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "    b_1 = -m_1 * dt_early\n",
    "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "    b_2 = -m_2 * dt_late\n",
    "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "    b_3 = -m_3 * dt_optimal\n",
    "\n",
    "    # Compare predicted and true conditions.\n",
    "    u = np.zeros(n)\n",
    "    for t in range(n):\n",
    "        if t <= t_sepsis + dt_late:\n",
    "            # TP\n",
    "            if is_septic and predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "            # FP\n",
    "            elif not is_septic and predictions[t]:\n",
    "                u[t] = u_fp\n",
    "            # FN\n",
    "            elif is_septic and not predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = 0\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "            # TN\n",
    "            elif not is_septic and not predictions[t]:\n",
    "                u[t] = u_tn\n",
    "\n",
    "    # Find total utility for patient.\n",
    "    return np.sum(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-317e45d4",
   "language": "python",
   "display_name": "PyCharm (sepsisprediction)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}