{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to find the most suitable K value for the KNN filling algorithm\n",
    "Since Logistic regression performs and worst among all algorithm, we don't use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn\n",
    "# Cross Validation Classification Accuracy\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing \n",
    "from sklearn.impute import KNNImputer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "#import faiss\n",
    "from numpy import isnan\n",
    "#import matplotlib.pyplot as plot\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn import tree\n",
    "import statistics\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# plotting errorbar graph for visual representation of the performance of the classifiers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.transforms import Affine2D\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "import ipynb.fs.full.our_functions_library as flib\n",
    "from our_evaluate_sepsis_score import evaluate_performance\n",
    "#from our_functions_library import *\n",
    "import numba\n",
    "from numba import jit, cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global initialization, get all the data from file and generate all the needed variable\n",
    "filename = 'raw_data/raw_data_all.csv' # use raw dataset\n",
    "#filename = 'data.csv' # use raw dataset\n",
    "originalData = read_csv(filename) # read csv data into DataFrame var raw\n",
    "print(\"Data size:\",originalData.shape)\n",
    "Original_Uniq_ID= np.unique(originalData['Patient_id']) \n",
    "Uniq_ID = Original_Uniq_ID.copy()\n",
    "print('Patient id size:',len(Uniq_ID))\n",
    "X_columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2','BaseExcess', 'HCO3', 'FiO2', 'pH', \n",
    "             'PaCO2', 'SaO2', 'AST', 'BUN','Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium','Bilirubin_total', 'TroponinI', 'Hct', \n",
    "             'Hgb', 'PTT', 'WBC','Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2','HospAdmTime',\n",
    "             'ICULOS', 'Patient_id', 'time']\n",
    "y_columns = ['Patient_id', 'SepsisLabel']\n",
    "\n",
    "# To test if these 3 columns is useful\n",
    "# originalData = originalData.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "# X_columns.remove('Unit1')\n",
    "# X_columns.remove( 'Unit2')\n",
    "# X_columns.remove( 'EtCO2')\n",
    "\n",
    "# Initialize the empty array X_train, X_test, y_train, y_test\n",
    "X_train = pd.DataFrame(columns = X_columns)\n",
    "X_test = pd.DataFrame(columns = X_columns)\n",
    "y_train = pd.DataFrame(columns = y_columns)\n",
    "y_test = pd.DataFrame(columns = y_columns)\n",
    "\n",
    "# store the X & y train/test in array for easy access\n",
    "globaldata = []\n",
    "globaldata.append(X_train)\n",
    "globaldata.append(y_train)\n",
    "globaldata.append(X_test)\n",
    "globaldata.append(y_test)\n",
    "\n",
    "# Below are the lists for KNN results\n",
    "\n",
    "KNN_UtilityScore_mean = []\n",
    "KNN_UtilityScore_std = []\n",
    "\n",
    "KNN_F1Score_mean = []\n",
    "KNN_F1Score_std = []\n",
    "\n",
    "KNN_auroc_mean = []\n",
    "KNN_auprc_mean = []\n",
    "\n",
    "KNN_accuracy_mean = []\n",
    "KNN_accuracy_std = []\n",
    "\n",
    "KNN_positiveprediction_mean = []\n",
    "KNN_sepsisLabelRatio_mean = []\n",
    "KNN_baseline_mean = [ ]\n",
    "\n",
    "KNN_total_Time= [ ]\n",
    "\n",
    "\n",
    "fillmethod =\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block will shuffle the id sets of patients in a fix manner, so for every time you run the code, the dataset and trainset are always the same.\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "if(np.array_equal(Original_Uniq_ID,Uniq_ID) ):\n",
    "    print(\"They are the same. Uniq id hasn't been shuffled, and it will be shuffeld now.\")   \n",
    "    random.shuffle(Uniq_ID)# randomly sorted the patient IDs    \n",
    "else:\n",
    "    print(\"They are not the same. Uniq_id has already been shuffled.\")    \n",
    "print(\"The original uniq id set is:\\n\",Original_Uniq_ID[1:20])\n",
    "print(\"Uniq id:************************************\")\n",
    "print(\"The fixed shuffelld id set, it should be 211 275 153 189 184 110 124  49 345...\\n\",Uniq_ID[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fill the missing data with one function\n",
    "# print(filename)\n",
    "# #print(originalData)\n",
    "# if (originalData.isnull().values.any()):\n",
    "#     print('There is data missing in the original data set')\n",
    "# dataByPatient = originalData.groupby('Patient_id') # data grouped by Patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_reset():\n",
    "    #KNN with different K size, \n",
    "    KNN_UtilityScore_mean.clear()\n",
    "    KNN_UtilityScore_std.clear()\n",
    "    KNN_F1Score_mean.clear()\n",
    "    KNN_F1Score_std.clear()\n",
    "    KNN_auroc_mean.clear()\n",
    "    KNN_auprc_mean.clear()\n",
    "    KNN_accuracy_mean.clear()\n",
    "    KNN_accuracy_std.clear()\n",
    "    KNN_positiveprediction_mean.clear()\n",
    "    KNN_sepsisLabelRatio_mean.clear() #array for storing the mean of the sepsislabel ratio (predict/true)\n",
    "    KNN_baseline_mean.clear()\n",
    "    KNN_total_Time.clear()\n",
    "    \n",
    "def generateTrainDataSet(test_patienIds, X_train, y_train):\n",
    "    train_data= originalData[~(originalData.Patient_id.isin(test_patienIds))]\n",
    "    X_train = train_data[X_columns]\n",
    "    y_train = train_data[y_columns]\n",
    "    return X_train, y_train\n",
    " \n",
    "def generateTestDataSet(patienIds, X_test, y_test):\n",
    "    #print(\"test_patienIds: \\n\", patienIds)\n",
    "    test_data = originalData[originalData['Patient_id'].isin(patienIds)]\n",
    "    X_test = test_data[X_columns]\n",
    "    y_test = test_data[y_columns]\n",
    "    return X_test, y_test\n",
    "\n",
    "# def CalculateSOFAScore(X_train_impute, X_test_impute):\n",
    "#     pass\n",
    "def WritePatientIdsToFile(idSets, KforKFold):\n",
    "    Twrite = time.time()\n",
    "    open(\"test.txt\", \"w\").close() # clear contents of existing file\n",
    "    for i in range(KforKFold):\n",
    "        splitted = \" \".join( repr(e) for e in idSets[i])\n",
    "        file1 = open(\"test.txt\",\"a\")\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.write(str(\"[\" +splitted+\"]\"))\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.close()\n",
    "    print(\"Time for writing id to file:\", round(time.time()- Twrite,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K-fold algorithm, Before this step the missing data has been filled, \n",
    "@jit(parallel=True)\n",
    "def KFold_patient(model, KforKFold=10,KforKNN=5, fillmethod=\"\"):\n",
    "    start = time.time()# time indicator for how long the Kfold func takes\n",
    "    X_train = globaldata[0]\n",
    "    y_train = globaldata[1]\n",
    "    X_test = globaldata[2]\n",
    "    y_test = globaldata[3]\n",
    "    print(\"Datasize\",len(Uniq_ID))\n",
    "    print(\"K Fold of \",KforKFold ,\" folds with KNN =\",KforKNN)\n",
    "    \n",
    "    # initialisation of the array for storing the different intermediate results\n",
    "    accuracy_model = []\n",
    "    F1Score_model = []\n",
    "    baseline_model= []\n",
    "    auroc_model = []\n",
    "    auprc_model = []\n",
    "    physio_accuracy_model = []\n",
    "    f_measure_model = []\n",
    "    utility_score_model = []\n",
    "    mean_train = 0\n",
    "    positivepredictions = []\n",
    "    predictionsRatio = []\n",
    "    tSplit = time.time()\n",
    "    #The unique id sets have been created and shuffled in a fix manner in the third block, you can just use it here and no more any other manipulation\n",
    "    idSets = np.array_split(Uniq_ID, KforKFold)# divide the ids into K groups\n",
    "    print(\"Time for splitting id to test:\", round(time.time()- tSplit,2))\n",
    "\n",
    "    #print(\"Patients number: \", len(Uniq_ID)) #print('idSets arrays',idSets)\n",
    "    #This for loop is for Kfold, calculating the results for K times\n",
    "    WritePatientIdsToFile(idSets, KforKFold)\n",
    "    \n",
    "    \n",
    "    for i in range(KforKFold):\n",
    "        start1 = time.time()\n",
    "        X_train, X_test, y_train, y_test = flib.clearAllDatasets(X_train, X_test, y_train, y_test) #first clear all the datasets\n",
    "        print(\"for the\",i+1,\"th iteration: \")#,idSets[i])   \n",
    "        \n",
    "        t1 = time.time()\n",
    "        X_test,y_test = generateTestDataSet(idSets[i], X_test, y_test)\n",
    "        print(\"Time for splitting id to test:\", round(time.time()- t1,2))\n",
    "        \n",
    "        t2= time.time()\n",
    "        X_train, y_train = generateTrainDataSet(idSets[i],X_train, y_train)\n",
    "        print(\"Time for splitting id to train:\", round(time.time()- t2,2))\n",
    "\n",
    "        #Now the train and test dataset is generated\n",
    "        #we can begin to train the model wit the training set and evaaulate the performance with the test sett   \n",
    "#         X_train = X_train.drop('time', 1) X_test = X_test.drop('time', 1)\n",
    "        X_train=X_train.astype('float64')\n",
    "        X_test=X_test.astype('float64')    \n",
    "        YTest_copy = y_test  # variable of joining the filled data (X) and Y (Train_output) \n",
    "        patientID_ytest = y_test['Patient_id']\n",
    "        y_train = y_train.drop('Patient_id', 1)\n",
    "        y_test = y_test.drop('Patient_id', 1)      \n",
    "        y_train=y_train.astype('float64')\n",
    "        y_test=y_test.astype('float64')\n",
    "        #print('YTest',YTest_copy.head())\n",
    "        \n",
    "        #fill the missing data\n",
    "        if X_train.isnull().values.any() or X_test.isnull().values.any() :\n",
    "            tKNN= time.time()\n",
    "            print(\"X_train or X_test contains NaN values, KNN/mean is performed.\")\n",
    "            #if there is missing value\n",
    "            #X_train_impute, X_test_impute, fillmethod = flib.KNNfilling(X_train, X_test, KforKNN, fillmethod)\n",
    "            X_train_impute, X_test_impute,fillmethod = flib.MeanFilling(X_train,X_test, fillmethod)\n",
    "            #check the missing data   \n",
    "            if np.isnan(X_train_impute).any() or np.isnan(X_test_impute).any() :\n",
    "                print(\"X_train_impute or X_test_impute still contains NaN values\")        \n",
    "            else:\n",
    "                print(\"X_train_impute or X_test_impute have all been filled \")\n",
    "            print(\"Time for NaN values filling: \", round(time.time()- tKNN,2) )\n",
    "        else:\n",
    "            print(\"X_train or X_test have all been filled: \")\n",
    "            X_train_impute = X_train\n",
    "            X_test_impute = X_test\n",
    "                 \n",
    "        #Scale the data， uncomment this part for logistic regression only, in other case, put them in comment\n",
    "        model_name = type(model).__name__\n",
    "        if (model_name == 'LogisticRegression'):\n",
    "            print(\"scaled!\")\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            scaler.fit(X_train_impute)\n",
    "            X_train_impute = scaler.transform(X_train_impute)\n",
    "            X_test_impute = scaler.transform(X_test_impute)\n",
    "\n",
    "        #fit the model and predict\n",
    "        tModel=time.time()\n",
    "        model.fit(X_train_impute, y_train)\n",
    "        y_predicted = model.predict(X_test_impute)   \n",
    "        y_predicted_probobility = model.predict_proba(X_test_impute) \n",
    "        #transfer the outpu and evalute it\n",
    "        y_labels = y_test.astype(int).to_numpy()\n",
    "        y_predicted = y_predicted.astype(int)\n",
    "        if(len(y_predicted_probobility[0]) ==2 ):\n",
    "            y_predicted_probobility =  y_predicted_probobility[:,1].round(4)\n",
    "        else:\n",
    "            print(\"columns of y_predicted_probobility is: \",len(y_predicted_probobility[0]))\n",
    "            y_predicted_probobility =  y_predicted_probobility.round(4)\n",
    "        print(\"Time for model fitting:\", round(time.time()- tSplit,2))\n",
    "\n",
    "        tUtil=time.time()\n",
    "        auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance(y_labels, y_predicted, y_predicted_probobility,patientID_ytest)\n",
    "#         print(\"\\nauroc\",round(auroc,4),\"auprc\",round(auprc,4),\"util_accuracy\",round(accuracy1,4))\n",
    "#         print(\"f_measure\",round(f_measure,4),\"utility_score\",round(utility_score,4))  \n",
    "        auroc_model.append(round(auroc,4))\n",
    "        auprc_model.append(round(auprc,4))\n",
    "        physio_accuracy_model.append(round(physio_accuracy,4))\n",
    "        f_measure_model.append(round(f_measure,4))\n",
    "        utility_score_model.append(round(utility_score,4))\n",
    "        positivepredictions.append(np.sum(y_predicted))\n",
    "        predictionsRatio.append(np.sum(y_predicted)/np.sum(y_test))\n",
    "        print(\"Time for model fitting:\", round(time.time()- tSplit,2))\n",
    "\n",
    "       \n",
    "        #take down the results\n",
    "        accuracy_model.append((accuracy_score(y_test, y_predicted, normalize=True)*100).round(2))\n",
    "        F1Score_model.append((f1_score(y_test, y_predicted)*100).round(2))\n",
    "        baseline_model.append(round( (1 - float(y_test.mean()) )*100 , 2 ))\n",
    "        print(\"\\ny_test size:\",y_test.shape, '1´s in y_test',y_test.sum())\n",
    "        #baseline_model.append(((1-y_test.mean())*100).round(2))\n",
    "        print(\"The number of 1 (SepsisLabel) in this prediction: \", np.sum(y_predicted))\n",
    "        end1 = time.time()\n",
    "        print(\"Time spent in this KFold iteration\",round(end1-start1,2),\"sec.\\n\")\n",
    "        print(\"******************************************************************\")\n",
    "    print('Accuracy model:', accuracy_model)\n",
    "    print('F1_score model:', F1Score_model)\n",
    "    print('Baseline model:', baseline_model)\n",
    "    \n",
    "    print(\"\\nEvaluation parameters of the utiltiy evaluation function:\")\n",
    "    print('auroc of model:', auroc_model)\n",
    "    print('auprc of model:', auprc_model)\n",
    "    print('Utility accuracy of model:', accuracy_model)\n",
    "    print('utility F1 of model:', f_measure_model)\n",
    "    print('Utility score of model:', utility_score_model)\n",
    "    \n",
    "    KNN_UtilityScore_mean.append(np.mean(utility_score_model))\n",
    "    KNN_UtilityScore_std.append(np.std(utility_score_model))\n",
    "    \n",
    "    KNN_F1Score_mean.append(np.mean(f_measure_model))\n",
    "    KNN_F1Score_std.append(np.std(f_measure_model))\n",
    "    \n",
    "    KNN_auroc_mean.append(np.mean(auroc_model))\n",
    "    KNN_auprc_mean.append(np.mean(auprc_model))\n",
    "    \n",
    "    KNN_accuracy_mean.append(np.mean(physio_accuracy_model))\n",
    "    KNN_accuracy_std.append(np.std(physio_accuracy_model))\n",
    "    \n",
    "    KNN_positiveprediction_mean.append(np.mean(positivepredictions))\n",
    "    KNN_sepsisLabelRatio_mean.append(np.mean(positivepredictions))\n",
    "    \n",
    "    KNN_baseline_mean.append(np.mean(baseline_model))\n",
    "    \n",
    "    totalTime=round(time.time()- start,2)\n",
    "    KNN_total_Time.append(totalTime)\n",
    "    \n",
    "    print(\"\\nTotal Time spent in  KFold function\",totalTime,\"sec.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Print_DATA():\n",
    "    print(\"KNN_UtilityScore_mean\", KNN_UtilityScore_mean)\n",
    "    print(\"KNN_UtilityScore_std\", KNN_UtilityScore_std)\n",
    "    print(\"KNN_F1Score_mean\", KNN_F1Score_mean)\n",
    "    print(\"KNN_F1Score_std\", KNN_F1Score_std)\n",
    "    print(\"KNN_auroc_mean\", KNN_auroc_mean)\n",
    "    print(\"KNN_auprc_mean\", KNN_auprc_mean)\n",
    "    print(\"KNN_accuracy_mean\", KNN_accuracy_mean)\n",
    "    print(\"KNN_accuracy_std\", KNN_accuracy_std)\n",
    "    print(\"KNN_positiveprediction_mean\", KNN_positiveprediction_mean)\n",
    "    print(\"KNN_baseline_mean\", KNN_baseline_mean)\n",
    "    print(\"KNN_total_time\", KNN_total_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is how to train the model\n",
    "#logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='saga', max_iter=1000,penalty='l1')\n",
    "logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='lbfgs', max_iter=1000)\n",
    "decisionTreeModel = tree.DecisionTreeClassifier(random_state=2)\n",
    "randomForestModel = RandomForestClassifier(random_state=2)\n",
    "XGBoostModel = XGBClassifier(random_state=2)\n",
    "AdaBoostModel = AdaBoostClassifier(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fill the missing data with one function\n",
    "print(filename)\n",
    "#print(originalData)\n",
    "if (originalData.isnull().values.any()):\n",
    "    print('Data isn´t all filled before K-Fold Func')\n",
    "    originalData1=originalData.copy()\n",
    "originalData,fillmethod = flib.linearFillingAll(originalData,1, True)#forwardfilling + fill other missing data with 0\n",
    "#dataByPatient = data.groupby('Patient_id') # data grouped by Patient_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(dataByPatient)\n",
    "# originalData.columns\n",
    "# model_name= decisionTreeModel.__class__.__name__\n",
    "# if (model_name == 'DecisionTreeClassifier'):\n",
    "#     print(\"oke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear filling (ffil + bfil+ interpolate) + decisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KFold_patient(decisionTreeModel,10,5)\n",
    "print(\"\\nKFold_patient for 350\\n\")\n",
    "KFold_patient(decisionTreeModel,10,350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_DATA()\n",
    "KNN_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear filling (ffil + bfil+ interpolate) + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold_patient(XGBoostModel,10,5)\n",
    "print(\"\\nKFold_patient for 350\\n\")\n",
    "KFold_patient(XGBoostModel,10,350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_DATA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Utiltity score for best K of 400, 1000 but now applied on 2000 patient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Logistic Regression\")\n",
    "KFold_patient(decisionTreeModel,10, 325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KFold_patient(decisionTreeModel,10, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_DATA_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold_patient(XGBoostModel,10, 355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy model: [94.39, 89.65, 77.34, 83.25, 93.85, 93.94, 88.73, 92.87, 97.38, 91.34]\n",
    "# F1_score model: [0.0, 2.53, 5.58, 2.94, 0.0, 0.0, 18.75, 24.66, 0.0, 11.11]\n",
    "# Baseline model: [98.47, 97.58, 98.21, 95.94, 98.42, 98.48, 95.81, 95.2, 98.54, 94.59]\n",
    "\n",
    "# Evaluation parameters of the utiltiy evaluation function:\n",
    "# auroc of model: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# auprc of model: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# Utility accuracy of model: [94.39, 89.65, 77.34, 83.25, 93.85, 93.94, 88.73, 92.87, 97.38, 91.34]\n",
    "# utility F1 of model: [0.0, 0.0253, 0.0558, 0.0294, 0.0, 0.0, 0.1875, 0.2466, 0.0, 0.1111]\n",
    "# Utility score of model: [-0.0923, -0.0583, 0.0301, -0.0208, -0.0837, -0.0857, 0.2554, 0.2509, -0.0229, 0.0562]\n",
    "\n",
    "# Total Time spent in  KFold function 109.83 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Decision tree\")\n",
    "#KFold_patient(randomForestModel,10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findBestKforKNN(logisticRegressionModel,10,1,10)\n",
    "# findBestKforKNN(XGBoostModel,10,110,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# findBestKforKNN(decisionTreeModel,10,1,10)\n",
    "# os.system('\\a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayCurrentResult(1,9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
