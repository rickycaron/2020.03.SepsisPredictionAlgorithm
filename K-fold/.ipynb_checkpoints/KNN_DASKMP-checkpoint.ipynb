{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to find the most suitable K value for the KNN filling algorithm\n",
    "Since Logistic regression performs and worst among all algorithm, we don't use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask_ml\n",
    "import joblib\n",
    "\n",
    "import dask\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.csvcsv)\n",
    "import sklearn as sk\n",
    "# import statsmodels.api as sm\n",
    "# # Cross Validation Classification Accuracy\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold,cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing \n",
    "# from sklearn.impute import KNNImputer\n",
    "# #import matplotlib.pyplot as plot\n",
    "# from matplotlib import pyplot\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn import tree\n",
    "# import statistics\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# plotting errorbar graph for visual representation of the performance of the classifiers\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.transforms import Affine2D\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "# import shutil\n",
    "# import os\n",
    "from our_evaluate_sepsis_score import evaluate_performance_dask\n",
    "import our_functions_library as flib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this once\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:53120</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>16.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:53120' processes=4 threads=16, memory=16.95 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this when the client has started\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (Delayed('int-cddda88f-a209-46e7-8108-d65ca812cdb6'), 43)\n",
      "Patient id size: 400\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400]\n"
     ]
    }
   ],
   "source": [
    "# dataType={'Age': 'int', 'Gender': bool, 'Patient_id': int, 'time': 'int8', 'SepsisLabel':'float16'}\n",
    "filename = '../../raw_data/raw_data_400.csv' # use raw dataset\n",
    "originalData = dd.read_csv(filename,dtype={'Gender':bool,'SepsisLabel':bool}) # read csv data into Dask DataFrame\n",
    "Uniq_ID= np.unique(originalData['Patient_id'])\n",
    "print(\"Data size:\",originalData.shape)\n",
    "print('Patient id size:',len(Uniq_ID))\n",
    "print(Uniq_ID)\n",
    "\n",
    "X_columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2','BaseExcess', 'HCO3', 'FiO2', 'pH', \n",
    "             'PaCO2', 'SaO2', 'AST', 'BUN','Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
    "             'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium','Bilirubin_total', 'TroponinI', 'Hct', \n",
    "             'Hgb', 'PTT', 'WBC','Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2','HospAdmTime',\n",
    "             'ICULOS', 'Patient_id', 'time']\n",
    "y_columns = ['Patient_id', 'SepsisLabel']\n",
    "\n",
    "# To test if these 3 columns is useful\n",
    "# originalData = originalData.drop(['Unit1', 'Unit2', 'EtCO2'], axis=1)\n",
    "# X_columns.remove('Unit1')\n",
    "# X_columns.remove( 'Unit2')\n",
    "# X_columns.remove( 'EtCO2')\n",
    "\n",
    "# Initialize the empty array X_train, X_test, y_train, y_test\n",
    "# X_train = dd.from_pandas(pd.DataFrame(columns = X_columns),npartitions=2)\n",
    "# X_test = dd.from_pandas(pd.DataFrame(columns = X_columns),npartitions=2)\n",
    "# y_train = dd.from_pandas(pd.DataFrame(columns = y_columns),npartitions=2)\n",
    "# y_test = dd.from_pandas(pd.DataFrame(columns = y_columns),npartitions=2)\n",
    "\n",
    "# Below are the lists for KNN results of different K value\n",
    "KNN_UtilityScore_mean = []\n",
    "KNN_UtilityScore_std = []\n",
    "KNN_F1Score_mean = []\n",
    "KNN_F1Score_std = []\n",
    "KNN_auroc_mean = []\n",
    "KNN_auprc_mean = []\n",
    "KNN_accuracy_mean = []\n",
    "KNN_accuracy_std = []\n",
    "KNN_positiveprediction_mean = []\n",
    "KNN_baseline_mean = [ ]\n",
    "KNN_total_Time= [ ]\n",
    "fillmethod =\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15348, 43)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalData.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 patient ids are the same. Uniq id hasn't been shuffled, and it will be shuffeld now.\n",
      "The original uniq id set is:\n",
      " [211 275 153 189 184 110 124  49 345  73 284  20  71 328 136 241 171 141\n",
      " 329 204]\n",
      "The fixed shuffelld id set, it should be 211 275 153 189 184 110 124  49...\n",
      " [211 275 153 189 184 110 124  49 345  73 284  20  71 328 136 241 171 141\n",
      " 329 204]\n"
     ]
    }
   ],
   "source": [
    "#This block will shuffle the id sets of patients in a fix manner, so for every time you run the code, the dataset and trainset are always the same.\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "if(np.array_equal(range(1,21),Uniq_ID[:20]) ):\n",
    "    print(\"The first 20 patient ids are the same. Uniq id hasn't been shuffled, and it will be shuffeld now.\")   \n",
    "    random.shuffle(Uniq_ID)# randomly sorted the patient IDs \n",
    "#     Uniq_ID = da.from_array(Uniq_ID)\n",
    "else:\n",
    "    print(\"They are not the same. Uniq_id has already been shuffled.\")    \n",
    "print(\"The original uniq id set is:\\n\",Uniq_ID[0:20])\n",
    "print(\"The fixed shuffelld id set, it should be 211 275 153 189 184 110 124  49...\\n\",Uniq_ID[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_reset():\n",
    "    KNN_UtilityScore_mean.clear()\n",
    "    KNN_UtilityScore_std.clear()\n",
    "    KNN_F1Score_mean.clear()\n",
    "    KNN_F1Score_std.clear()\n",
    "    KNN_auroc_mean.clear()\n",
    "    KNN_auprc_mean.clear()\n",
    "    KNN_accuracy_mean.clear()\n",
    "    KNN_accuracy_std.clear()\n",
    "    KNN_positiveprediction_mean.clear()\n",
    "    KNN_baseline_mean.clear()\n",
    "    KNN_total_Time.clear()\n",
    "    \n",
    "\n",
    "def generateTrainDataSet(test_patienIds):\n",
    "    train_data= originalData[~(originalData.Patient_id.isin(test_patienIds))]\n",
    "    X_train = train_data[X_columns]\n",
    "    y_train = train_data[y_columns] \n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def generateTestDataSet(patienIds):\n",
    "    print(\"test_patienIds: \\n\", patienIds)\n",
    "    test_data = originalData[originalData['Patient_id'].isin(patienIds)]\n",
    "    X_test = test_data[X_columns]\n",
    "    y_test = test_data[y_columns]\n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "def KNNfilling(trainData,testData,K= 5):\n",
    "    imputer = KNNImputer(n_neighbors = K)\n",
    "    #imputer = FaissKNeighbors(k=K)\n",
    "    imputer.fit(trainData)\n",
    "    x_train_impute=imputer.transform(trainData).round(3).astype('float32')\n",
    "    x_test_impute=imputer.transform(testData).round(3).astype('float32')\n",
    "#     x_train_impute=imputer.transform(trainData)\n",
    "#     x_test_impute=imputer.transform(testData)\n",
    "    fillmethod= \"KnnFill\"\n",
    "    return x_train_impute, x_test_impute,fillmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for 1 fold in Kfold\n",
    "def OneFold_patient_MP(i,model,KforKNN,idSets,fillmethod):\n",
    "#     start1 = time.time()\n",
    "#     clearAllDatasets()\n",
    "# #     X_train, X_test, y_train, y_test = flib.clearAllDatasets(X_train, X_test, y_train, y_test) #first clear all the datasets\n",
    "#     print(\"for the\",i+1,\"th iteration\",idSets[i])   \n",
    "#     t1 = time.time()\n",
    "#     X_test,y_test = generateTestDataSet(idSets[i])\n",
    "#     print(\"Time for splitting id to test:\", round(time.time()- t1,2))\n",
    "#     t2= time.time()\n",
    "#     for j in range(KforKFold):\n",
    "#         if j != i:\n",
    "#              X_train, y_train = generateTrainDataSet(idSets[j])\n",
    "#     print(\"Time for splitting id to train:\", round(time.time()- t2,2))\n",
    "#     flib.printDataset(X_train, X_test, y_train, y_test)\n",
    "#     #Now the train and test dataset is generated\n",
    "#     #we can begin to train the model wit the training set and evaaulate the performance with the test sett   \n",
    "# #         X_train = X_train.drop('time', 1) X_test = X_test.drop('time', 1)\n",
    "#     X_train=X_train.astype('float64')\n",
    "#     X_test=X_test.astype('float64')    \n",
    "#     #YTest_copy = y_test  # variable of joining the filled data (X) and Y (Train_output) \n",
    "#     patientID_ytest = y_test['Patient_id']\n",
    "#     y_train = y_train.drop('Patient_id', 1)\n",
    "#     y_test = y_test.drop('Patient_id', 1)      \n",
    "#     y_train=y_train.astype('float64')\n",
    "#     y_test=y_test.astype('float64')\n",
    "#     #print('YTest',YTest_copy.head())\n",
    "\n",
    "#     #fill the missing data\n",
    "#     if X_train.isnull().values.any() or X_test.isnull().values.any() :\n",
    "#         print(\"X_train or X_test contains NaN values, KNN/mean is performed.\")\n",
    "#         #if there is missing value\n",
    "#         X_train_impute, X_test_impute, fillmethod = flib.KNNfilling(X_train, X_test, KforKNN, fillmethod)\n",
    "#         #X_train_impute, X_test_impute,fillmethod = flib.MeanFilling(X_train,X_test, fillmethod)\n",
    "#         #check the missing data   \n",
    "#         if np.isnan(X_train_impute).any() or np.isnan(X_test_impute).any() :\n",
    "#             print(\"X_train_impute or X_test_impute still contains NaN values\")        \n",
    "#         else:\n",
    "#             print(\"X_train_impute or X_test_impute have all been filled \")  \n",
    "#     else:\n",
    "#         print(\"X_train or X_test have all been filled \")\n",
    "#         X_train_impute = X_train\n",
    "#         X_test_impute = X_test\n",
    "    \n",
    "\n",
    "#     #Scale the data， uncomment this part for logistic regression only, in other case, put them in comment\n",
    "# #     model_name = type(model).__name__\n",
    "# #     if (model_name == 'LogisticRegression'):\n",
    "# #         print(\"scaled!\")\n",
    "# #         scaler = preprocessing.StandardScaler()\n",
    "# #         scaler.fit(X_train_impute)\n",
    "# #         X_train_impute = scaler.transform(X_train_impute)\n",
    "# #         X_test_impute = scaler.transform(X_test_impute)\n",
    "\n",
    "#     #fit the model and predict\n",
    "#     model.fit(X_train_impute, y_train)\n",
    "#     y_predicted = model.predict(X_test_impute)   \n",
    "#     y_predicted_probobility = model.predict_proba(X_test_impute) \n",
    "#     #transfer the outpu and evalute it\n",
    "#     y_labels = y_test.astype(int).to_numpy()\n",
    "#     y_predicted = y_predicted.astype(int)\n",
    "#     y_predicted_probobility =  y_predicted_probobility[:,1].round(4)\n",
    "#     #print(y_labels)      \n",
    "#     auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance(y_labels, y_predicted, y_predicted_probobility,patientID_ytest)\n",
    "# #         print(\"\\nauroc\",round(auroc,4),\"auprc\",round(auprc,4),\"util_accuracy\",round(accuracy1,4))\n",
    "# #         print(\"f_measure\",round(f_measure,4),\"utility_score\",round(utility_score,4))  \n",
    "    print(\"for the\",i+1,\"th iteration: \")\n",
    "    tfold = time.time()\n",
    "    X_test,y_test = generateTestDataSet(idSets[i])\n",
    "    print(\"Time for splitting id to test dataset:\", round(time.time()- t1,3))\n",
    "    t1 = time.time()\n",
    "    X_train, y_train = generateTrainDataSet(idSets[i])\n",
    "    print(\"Time for splitting id to train dataset:\", round(time.time()- t1,3))\n",
    "\n",
    "    #Now the train and test dataset is generated \n",
    "    patientID_ytest = y_test['Patient_id']\n",
    "    y_test = y_test.drop('Patient_id', 1) \n",
    "    y_train = y_train.drop('Patient_id', 1)\n",
    "#     X_train=X_train.astype('float16')\n",
    "#     X_test=X_test.astype('float16') \n",
    "    X_train=X_train.astype('float32')\n",
    "    X_test=X_test.astype('float32') \n",
    "    y_train=y_train.astype('bool')\n",
    "    y_test=y_test.astype('bool')\n",
    "    print(X_train)\n",
    "    \n",
    "    #fill the missing data\n",
    "    if X_train.isnull().values.any() or X_test.isnull().values.any() :\n",
    "        t1= time.time()\n",
    "        print(\"X_train or X_test contains NaN values, KNN is performed.\")\n",
    "#         X_train, X_test,fillmethod = flib.MeanFilling(X_train,X_test, fillmethod)  \n",
    "        X_train, X_test, fillmethod = flib.KNNfilling(X_train, X_test, KforKNN)\n",
    "        print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "        print(\"Time for NaN values filling: \", round(time.time()- t1,2) )\n",
    "        X_train = dd.from_array(X_train)\n",
    "        X_test = dd.from_array(X_test)\n",
    "#         if np.isnan(X_train).any() or np.isnan(X_test).any() :\n",
    "#             print(\"X_train_impute or X_test_impute still contains NaN values\")          \n",
    "    print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "    \n",
    "    \n",
    "    if ( type(model).__name__ == 'LogisticRegression'):  #Scale the data for logistic regression\n",
    "        t1 = time.time()\n",
    "        scaler = dask_ml.preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        print(\"Standard Scaling time: \", round(time.time()- t1,2) )\n",
    "\n",
    "               \n",
    "    #fit the model and predict\n",
    "    with joblib.parallel_backend(\"dask\",scatter=[X_train, y_train]):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predicted = model.predict(X_test).astype(bool)   \n",
    "        y_predicted_probobility = model.predict_proba(X_test).astype('float32') \n",
    "    y_labels = y_test['SepsisLabel'].reset_index(drop=True) \n",
    "    del X_train, X_test, y_train\n",
    "\n",
    "    print(\"columns of y_predicted_probobility is: \",len(y_predicted_probobility[0]))\n",
    "    if(len(y_predicted_probobility[0]) ==2 ):\n",
    "        y_predicted_probobility =  y_predicted_probobility[:,1].round(4)\n",
    "    elif(len(y_predicted_probobility[0]) == 1):     \n",
    "        y_predicted_probobility =  y_predicted_probobility.round(4)\n",
    "    \n",
    "    auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance_dask(y_labels.compute(), y_predicted, y_predicted_probobility,patientID_ytest.compute(),idSets[i]) \n",
    "    del y_labels, y_predicted_probobility    \n",
    "    result = { \"auroc\": round(auroc,4), \"auprc\": round(auprc,4), \"f_measure\": round(f_measure,4),\n",
    "                \"physio_accuracy\": round(physio_accuracy,4),\"utility_score\": round(utility_score,4), \n",
    "                \"positiveprediction\": np.sum(y_predicted), \n",
    "            \"baseline\": round( (1 - y_test.mean().compute() )*100 , 2 )}\n",
    "    print(\"\\ny_test size:\",y_test.shape, '1´s in y_test',y_test.sum())\n",
    "    del  y_test\n",
    "    print(\"Time spent in \",i,\"th Fold iteration\",round(end1-start1,2),\"sec.\\n\")\n",
    "    print(\"******************************************************************\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K-fold algorithm using multi processes\n",
    "def KFold_patient_DASKMP(model, KforKFold=10,KforKNN=5, fillmethod=\"\"):\n",
    "    start = time.time()# time indicator for how long the Kfold func takes  \n",
    "#     global X_train, X_test, y_train, y_test\n",
    "    print(\"Datasize\",len(Uniq_ID) ,\"with K Fold of \",KforKFold ,\" folds filling by KNN =\",KforKNN)\n",
    "    # initialisation of the array for storing the different intermediate results\n",
    "    auroc_model = []\n",
    "    auprc_model = []\n",
    "    f_measure_model = []\n",
    "    physio_accuracy_model = []\n",
    "    utility_score_model = []\n",
    "    positivepredictions = []\n",
    "    baseline_model= []\n",
    "    #The unique id sets have been created and shuffled in a fix manner in the third block, you can just use it here and no more any other manipulation\n",
    "    idSets = np.array_split(Uniq_ID, KforKFold)# divide the ids into K groups\n",
    "\n",
    "    #write the idset data into a file\n",
    "    open(\"test.txt\", \"w\").close() # clear contents of existing file\n",
    "    for i in range(KforKFold):\n",
    "        splitted = \" \".join( repr(e) for e in idSets[i])\n",
    "        file1 = open(\"test.txt\",\"a\")\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.write(str(\"[\" +splitted+\"]\"))\n",
    "        file1.write(\"\\n\\n\")\n",
    "        file1.close()\n",
    "    print(\"Time for writing id to file:\", round(time.time()- start,3))\n",
    "\n",
    "    folds = list(range(0,KforKFold))\n",
    "    results = client.submit(OneFold_patient_MP,[(fold, model, KforKNN,idSets,fillmethod) for fold in folds])\n",
    "#     with ProcessPoolExecutor (max_workers = 10) as executor:  \n",
    "#         results = executor.submit(OneFold_patient_MP, folds)\n",
    "        \n",
    "#         for result in results:\n",
    "#             print(result)\n",
    "\n",
    "#     pool = mp.Pool()#here in the parentthses should be 10, but since my mac only has 8, so leave it blank \n",
    "#     results = pool.starmap(OneFold_patient_MP, [(fold, model, KforKFold, KforKNN,idSets,fillmethod) for fold in folds])\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    for r in results:\n",
    "        result = r.result()\n",
    "        auroc_model.append(result[\"auroc\"])\n",
    "        auprc_model.append(result[\"auprc\"])\n",
    "        f_measure_model.append(result[\"f_measure\"])\n",
    "        physio_accuracy_model.append(result[\"physio_accuracy\"])  \n",
    "        utility_score_model.append(result[\"utility_score\"])\n",
    "        positivepredictions.append(result[\"positiveprediction\"])\n",
    "        baseline_model.append(result[\"baseline\"])        \n",
    "    print(\"\\nEvaluation parameters of the utiltiy evaluation function:\")\n",
    "    print('auroc of model:', auroc_model)\n",
    "    print('auprc of model:', auprc_model)\n",
    "    print('utility F1 of model:', f_measure_model)\n",
    "    print('Utility accuracy of model:', accuracy_model) \n",
    "    print('Utility score of model:', utility_score_model)\n",
    "    print(\"Positive Prediction: \", positivepredictions)\n",
    "    print('Baseline model:', baseline_model)  \n",
    "    KNN_auroc_mean.append(np.mean(auroc_model))\n",
    "    KNN_auprc_mean.append(np.mean(auprc_model))\n",
    "    KNN_F1Score_mean.append(np.mean(f_measure_model))\n",
    "    KNN_F1Score_std.append(np.std(f_measure_model))\n",
    "    KNN_accuracy_mean.append(np.mean(physio_accuracy_model))\n",
    "    KNN_accuracy_std.append(np.std(physio_accuracy_model))\n",
    "    KNN_UtilityScore_mean.append(np.mean(utility_score_model))\n",
    "    KNN_UtilityScore_std.append(np.std(utility_score_model))\n",
    "    KNN_positiveprediction_mean.append(np.mean(positivepredictions))\n",
    "    KNN_baseline_mean.append(np.mean(baseline_model))\n",
    "    \n",
    "    totalTime=round(time.time()- start,2)\n",
    "    KNN_total_Time.append(totalTime)\n",
    "    print(\"\\nTotal Time spent in  KFold function\",totalTime,\"sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasize 400 with K Fold of  10  folds filling by KNN = 5\n",
      "Time for writing id to file: 0.003\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Future' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9416f4fdadcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKFold_patient_DASKMP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecisionTreeModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-ebb66a9e9c1f>\u001b[0m in \u001b[0;36mKFold_patient_DASKMP\u001b[1;34m(model, KforKFold, KforKNN, fillmethod)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#     pool.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#     pool.join()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mr1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Future' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "KFold_patient_DASKMP(decisionTreeModel,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model=sk.tree.DecisionTreeClassifier(random_state=2)\n",
    "# KforKFold=10\n",
    "# KforKNN=5\n",
    "# fillmethod=\"\"\n",
    "# # Code for K-fold algorithm \n",
    "# # def KFold_patient(model, KforKFold=10,KforKNN=5, fillmethod=\"\"):\n",
    "# global X_train, X_test, y_train, y_test\n",
    "# print(\"Datasize\",len(Uniq_ID) ,\"with K Fold of \",KforKFold ,\" folds filling by KNN =\",KforKNN)\n",
    "\n",
    "# # initialisation of the array for storing the different intermediate results\n",
    "# auroc_model = []\n",
    "# auprc_model = []\n",
    "# physio_accuracy_model = []\n",
    "# f_measure_model = []\n",
    "# utility_score_model = []\n",
    "# positivepredictions = []\n",
    "# baseline_model= []\n",
    "# #The unique id sets have been created and shuffled in a fix manner in the third block, you can just use it here and no more any other manipulation\n",
    "# idSets = np.array_split(Uniq_ID, KforKFold)# divide the ids into K groups\n",
    "# #This for loop is for Kfold, calculating the results for K times\n",
    "\n",
    "# start = time.time()# time indicator for how long the Kfold func takes\n",
    "\n",
    "# #write the idset data into a file\n",
    "# open(\"test.txt\", \"w\").close() # clear contents of existing file\n",
    "# for i in range(KforKFold):\n",
    "#     splitted = \" \".join( repr(e) for e in idSets[i])\n",
    "#     file1 = open(\"test.txt\",\"a\")\n",
    "#     file1.write(\"\\n\\n\")\n",
    "#     file1.write(str(\"[\" +splitted+\"]\"))\n",
    "#     file1.write(\"\\n\\n\")\n",
    "#     file1.close()\n",
    "# print(\"Time for writing id to file:\", round(time.time()- start,3))\n",
    "\n",
    "# # for i in range(KforKFold):\n",
    "# for i in range(1):\n",
    "#     print(\"for the\",i+1,\"th iteration: \")\n",
    "#     tfold = time.time()\n",
    "#     X_test,y_test = generateTestDataSet(idSets[i])\n",
    "#     print(\"Time for splitting id to test dataset:\", round(time.time()- t1,3))\n",
    "#     t1 = time.time()\n",
    "#     X_train, y_train = generateTrainDataSet(idSets[i])\n",
    "#     print(\"Time for splitting id to train dataset:\", round(time.time()- t1,3))\n",
    "\n",
    "#     #Now the train and test dataset is generated \n",
    "#     patientID_ytest = y_test['Patient_id']\n",
    "#     y_test = y_test.drop('Patient_id', 1) \n",
    "#     y_train = y_train.drop('Patient_id', 1)\n",
    "# #     X_train=X_train.astype('float16')\n",
    "# #     X_test=X_test.astype('float16') \n",
    "#     X_train=X_train.astype('float32')\n",
    "#     X_test=X_test.astype('float32') \n",
    "#     y_train=y_train.astype('bool')\n",
    "#     y_test=y_test.astype('bool')\n",
    "#     print(X_train)\n",
    "    \n",
    "#     #fill the missing data\n",
    "#     if X_train.isnull().values.any() or X_test.isnull().values.any() :\n",
    "#         t1= time.time()\n",
    "#         print(\"X_train or X_test contains NaN values, KNN is performed.\")\n",
    "# #         X_train, X_test,fillmethod = flib.MeanFilling(X_train,X_test, fillmethod)  \n",
    "#         X_train, X_test, fillmethod = flib.KNNfilling(X_train, X_test, KforKNN)\n",
    "#         print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "#         print(\"Time for NaN values filling: \", round(time.time()- t1,2) )\n",
    "#         X_train = dd.from_array(X_train)\n",
    "#         X_test = dd.from_array(X_test)\n",
    "# #         if np.isnan(X_train).any() or np.isnan(X_test).any() :\n",
    "# #             print(\"X_train_impute or X_test_impute still contains NaN values\")          \n",
    "#     print(\"The type of X_train after KNN filling is\",type(X_train))\n",
    "    \n",
    "    \n",
    "#     if ( type(model).__name__ == 'LogisticRegression'):  #Scale the data for logistic regression\n",
    "#         t1 = time.time()\n",
    "#         scaler = dask_ml.preprocessing.StandardScaler()\n",
    "#         scaler.fit(X_train)\n",
    "#         X_train = scaler.transform(X_train)\n",
    "#         X_test = scaler.transform(X_test)\n",
    "#         print(\"Standard Scaling time: \", round(time.time()- t1,2) )\n",
    "\n",
    "               \n",
    "#     #fit the model and predict\n",
    "#     with joblib.parallel_backend(\"dask\",scatter=[X_train, y_train]):\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_predicted = model.predict(X_test).astype(bool)   \n",
    "#         y_predicted_probobility = model.predict_proba(X_test).astype('float32') \n",
    "#     y_labels = y_test['SepsisLabel'].reset_index(drop=True) \n",
    "#     del X_train, X_test, y_train\n",
    "\n",
    "#     print(\"columns of y_predicted_probobility is: \",len(y_predicted_probobility[0]))\n",
    "#     if(len(y_predicted_probobility[0]) ==2 ):\n",
    "#         y_predicted_probobility =  y_predicted_probobility[:,1].round(4)\n",
    "#     elif(len(y_predicted_probobility[0]) == 1):     \n",
    "#         y_predicted_probobility =  y_predicted_probobility.round(4)\n",
    "    \n",
    "#     auroc, auprc, physio_accuracy, f_measure, utility_score = evaluate_performance_dask(y_labels.compute(), y_predicted, y_predicted_probobility,patientID_ytest.compute(),idSets[i]) \n",
    "#     del y_labels, y_predicted_probobility\n",
    "#     auroc_model.append(round(auroc,4))\n",
    "#     auprc_model.append(round(auprc,4))\n",
    "#     physio_accuracy_model.append(round(physio_accuracy,4))\n",
    "#     f_measure_model.append(round(f_measure,4))\n",
    "#     utility_score_model.append(round(utility_score,4))\n",
    "#     positivepredictions.append(np.sum(y_predicted))        \n",
    "#     baseline_model.append(round( (1 - y_test.mean().compute() )*100 , 2 ))\n",
    "#     print(\"\\ny_test size:\",y_test.shape, '1´s in y_test',y_test.sum())\n",
    "#     del  y_test\n",
    "#     print(\"Time spent in this KFold iteration\",round(time.time()-tfold,2),\"sec.\\n\")\n",
    "#     print(\"******************************************************************\")    \n",
    "# print(\"\\nEvaluation parameters of the utiltiy evaluation function:\")\n",
    "# print('auroc of model:', auroc_model)\n",
    "# print('auprc of model:', auprc_model)\n",
    "# print('utility F1 of model:', f_measure_model)\n",
    "# print('Utility accuracy of model:', physio_accuracy_model) \n",
    "# print('Utility score of model:', utility_score_model)\n",
    "# print(\"Positive Prediction: \", positivepredictions)\n",
    "# print('Baseline model:', baseline_model)  \n",
    "# KNN_auroc_mean.append(np.mean(auroc_model))\n",
    "# KNN_auprc_mean.append(np.mean(auprc_model))\n",
    "# KNN_F1Score_mean.append(np.mean(f_measure_model))\n",
    "# KNN_F1Score_std.append(np.std(f_measure_model))\n",
    "# KNN_accuracy_mean.append(np.mean(physio_accuracy_model))\n",
    "# KNN_accuracy_std.append(np.std(physio_accuracy_model))\n",
    "# KNN_UtilityScore_mean.append(np.mean(utility_score_model))\n",
    "# KNN_UtilityScore_std.append(np.std(utility_score_model))\n",
    "# KNN_positiveprediction_mean.append(np.mean(positivepredictions))\n",
    "# KNN_baseline_mean.append(np.mean(baseline_model))\n",
    "# totalTime=round(time.time()- start,2)\n",
    "# KNN_total_Time.append(totalTime)\n",
    "# print(\"\\nTotal Time spent in  KFold function\",totalTime,\"sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(type(y_labels.compute()))\n",
    "# print(y_labels)\n",
    "# y_labels.compute().rename(\"labels\")\n",
    "y_test.compute().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(getsizeof(X_train))\n",
    "print(getsizeof(X_test))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# X_test.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"X_train size is: \",X_train.size * X_train.itemsize)\n",
    "# print(\"X_test size is: \",X_test.size * X_test.itemsize)\n",
    "print(\"X_train size is: \",X_train.memory_usage(deep=True).sum().compute())\n",
    "print(\"X_test size is: \",X_test.memory_usage(deep=True).sum().compute())\n",
    "print(\"y_train size is: \",y_train.memory_usage(deep=True).sum().compute())\n",
    "print(\"y_test size is: \",y_test.memory_usage(deep=True).sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(y_labels)\n",
    "print(y_labels)\n",
    "print(y_predicted)\n",
    "print(type(y_predicted))\n",
    "print(y_predicted_probobility)\n",
    "print(type(y_predicted_probobility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.loc[0:0].compute()\n",
    "# X_train[0:0].compute()\n",
    "# del y_train,y_test,X_test\n",
    "# del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_DATA():\n",
    "    print(\"Numbers of K:\",len(KNN_accuracy_mean))\n",
    "    print(\"KNN_UtilityScore_mean\", KNN_UtilityScore_mean)\n",
    "    print(\"KNN_UtilityScore_std\", KNN_UtilityScore_std)\n",
    "    print(\"KNN_F1Score_mean\", KNN_F1Score_mean)\n",
    "    print(\"KNN_F1Score_std\", KNN_F1Score_std)\n",
    "    print(\"KNN_auroc_mean\", KNN_auroc_mean)\n",
    "    print(\"KNN_auprc_mean\", KNN_auprc_mean)\n",
    "    print(\"KNN_accuracy_mean\", KNN_accuracy_mean)\n",
    "    print(\"KNN_accuracy_std\", KNN_accuracy_std)\n",
    "    print(\"KNN_positiveprediction_mean\", KNN_positiveprediction_mean)\n",
    "    print(\"KNN_baseline_mean\", KNN_baseline_mean)\n",
    "    print(\"KNN_total_time\", KNN_total_Time)      \n",
    "\n",
    "# def findBestKforKNN(model, KforKFold=10,KforKNNstart=1,KforKNNend=10, stepsize=5):\n",
    "#     if(KforKNNend<=1):\n",
    "#         print(\"K must be a interger larger than 1\")\n",
    "#         return\n",
    "#     KNN_reset()\n",
    "#     step=stepsize\n",
    "#     for i in range(KforKNNstart,KforKNNend+1,step) : \n",
    "#         print(\"KNN of K = \",i)    \n",
    "#         KFold_patient(model,10,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='saga', max_iter=1000,penalty='l1')\n",
    "logisticRegressionModel = LogisticRegression(C=0.1,random_state=2,solver='lbfgs', max_iter=1000)\n",
    "decisionTreeModel = tree.DecisionTreeClassifier(random_state=2)\n",
    "randomForestModel = RandomForestClassifier(random_state=2)\n",
    "XGBoostModel = XGBClassifier(random_state=2, n_jobs=4)\n",
    "#AdaBoostModel = AdaBoostClassifier(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing data with one function\n",
    "print(filename)\n",
    "#print(originalData)\n",
    "if (originalData.isnull().values.any()):\n",
    "    print('Data isn´t all filled before K-Fold Func')\n",
    "#data,fillmethod = flib.linearFillingAll(originalData,1, True)#forwardfilling + fill other missing data with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KFold_patient(decisionTreeModel,10,5)\n",
    "Print_DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasize 400 with K Fold of  10  folds filling by KNN = 5\n",
      "Time for writing id to file: 0.003\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Future' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9416f4fdadcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKFold_patient_DASKMP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecisionTreeModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-706a3d1d1754>\u001b[0m in \u001b[0;36mKFold_patient_DASKMP\u001b[1;34m(model, KforKFold, KforKNN, fillmethod)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#     pool.join()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mauroc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"auroc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Future' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def evaluate_performance_dask(labels,predictions,probabilities,patientids,idSet):\n",
    "#     # print(\"****************************************************************************\")\n",
    "#     # print(type(labels))\n",
    "#     # print(labels)\n",
    "#     # print(\"*****************************\")\n",
    "#     # print(type(predictions))\n",
    "#     # print(predictions)\n",
    "#     # print(\"*****************************\")\n",
    "#     # print(type(patientids))\n",
    "#     # print(patientids)\n",
    "#     # print(\"*****************************\")\n",
    "#     # print(type(probabilities))\n",
    "#     # print(probabilities)\n",
    "#     # print(\"*****************************\")\n",
    "#     if ( not( len(labels) == len(predictions) == len(probabilities) )):\n",
    "#         print(\"The predicted data is not the same in size! Evaluation function will stop here!\" )\n",
    "#         return\n",
    "#     # Set parameters.\n",
    "#     label_header       = 'SepsisLabel'\n",
    "#     prediction_header  = 'PredictedLabel'\n",
    "#     probability_header = 'PredictedProbability'\n",
    "#     dt_early   = -12\n",
    "#     dt_optimal = -6\n",
    "#     dt_late    = 3\n",
    "#     max_u_tp = 1\n",
    "#     min_u_fn = -2\n",
    "#     u_fp     = -0.05\n",
    "#     u_tn     = 0\n",
    "    \n",
    "#     auroc, auprc        = compute_auc(labels, probabilities)\n",
    "#     accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "    \n",
    "#     patientids = patientids.astype('int')\n",
    "#     patientdata = patientids.reset_index()\n",
    "# #     print(patientids)\n",
    "# #     patientdata = patientdata.join(pd.Series(labels.flatten(),name='labels'))\n",
    "#     patientdata = patientdata.join(labels.rename('labels'))\n",
    "#     patientdata = patientdata.join(pd.Series(predictions.flatten(),name='predictions'))\n",
    "#     patientdata = patientdata.drop(\"index\",1)\n",
    "\n",
    "#     # print(\"After the joining:\")\n",
    "#     # print(patientdata.columns)\n",
    "#     # print(patientdata)\n",
    "\n",
    "# #     uniq_ids = np.unique(patientdata['Patient_id']) \n",
    "#     num_patients = len(idSet)\n",
    "#     dataByPatient = patientdata.groupby('Patient_id')\n",
    "\n",
    "#     #print(uniq_ids)\n",
    "    \n",
    "#     # Compute utility.\n",
    "#     observed_utilities = np.zeros(num_patients)\n",
    "#     best_utilities     = np.zeros(num_patients)\n",
    "#     worst_utilities    = np.zeros(num_patients)\n",
    "#     inaction_utilities = np.zeros(num_patients)\n",
    "\n",
    "#     for i in range(num_patients):\n",
    "#         k = idSet[i]\n",
    "#         # print(\" for the patient of id:\",k)\n",
    "#         # print(dataByPatient.get_group(k))\n",
    "\n",
    "#         labels = dataByPatient.get_group(k).loc[:, 'labels'].to_numpy()\n",
    "#         num_rows          = len(labels)\n",
    "#         observed_predictions = dataByPatient.get_group(k).loc[:, 'predictions'].to_numpy()\n",
    "\n",
    "#         #print(\"labels\",labels)\n",
    "#         #print(\"observered_prediction\",observed_predictions)\n",
    "\n",
    "#         best_predictions     = np.zeros(num_rows)\n",
    "#         worst_predictions    = np.zeros(num_rows)\n",
    "#         inaction_predictions = np.zeros(num_rows)\n",
    "\n",
    "#         if np.any(labels):\n",
    "#             t_sepsis = np.argmax(labels) - dt_optimal\n",
    "#             best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, num_rows)] = 1\n",
    "#         worst_predictions = 1 - best_predictions\n",
    "\n",
    "#         observed_utilities[i] = compute_prediction_utility(labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "#         best_utilities[i]     = compute_prediction_utility(labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "#         worst_utilities[i]    = compute_prediction_utility(labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "#         inaction_utilities[i] = compute_prediction_utility(labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "#     unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "#     unnormalized_best_utility     = np.sum(best_utilities)\n",
    "#     unnormalized_worst_utility    = np.sum(worst_utilities)\n",
    "#     unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "#     normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "#     return auroc, auprc, accuracy, f_measure, normalized_observed_utility\n",
    "\n",
    "\n",
    "# # The load_column function loads a column from a table.\n",
    "# #\n",
    "# # Inputs:\n",
    "# #   'filename' is a string containing a filename.\n",
    "# #\n",
    "# #   'header' is a string containing a header.\n",
    "# #\n",
    "# # Outputs:\n",
    "# #   'column' is a vector containing a column from the file with the given\n",
    "# #   header.\n",
    "# #\n",
    "# # Example:\n",
    "# #   Omitted.\n",
    "\n",
    "# def load_column(filename: object, header: object, delimiter: object) -> object:\n",
    "#     column = []\n",
    "#     with open(filename, 'r') as f:\n",
    "#         for i, l in enumerate(f):\n",
    "#             arrs = l.strip().split(delimiter)\n",
    "#             if i == 0:\n",
    "#                 try:\n",
    "#                     j = arrs.index(header)\n",
    "#                 except:\n",
    "#                     raise Exception('{} must contain column with header {} containing numerical entries.'.format(filename, header))\n",
    "#             else:\n",
    "#                 if len(arrs[j]):\n",
    "#                     column.append(float(arrs[j]))\n",
    "#     return np.array(column)\n",
    "\n",
    "# \"\"\n",
    "# # The compute_auc function computes AUROC and AUPRC as well as other summary\n",
    "# # statistics (TP, FP, FN, TN, TPR, TNR, PPV, NPV, etc.) that can be exposed\n",
    "# # from this function.\n",
    "# #\n",
    "# # Inputs:\n",
    "# #   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "# #   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "# #   septic at time i.\n",
    "# #\n",
    "# #   'predictions' is a probability vector, where predictions[i] gives the\n",
    "# #   predicted probability that the patient is septic at time i.  Note that there\n",
    "# #   must be a prediction for every label, i.e, len(labels) ==\n",
    "# #   len(predictions).\n",
    "# #\n",
    "# # Outputs:\n",
    "# #   'auroc' is a scalar that gives the AUROC of the algorithm using its\n",
    "# #   predicted probabilities, where specificity is interpolated for intermediate\n",
    "# #   sensitivity values.\n",
    "# #\n",
    "# #   'auprc' is a scalar that gives the AUPRC of the algorithm using its\n",
    "# #   predicted probabilities, where precision is a piecewise constant function of\n",
    "# #   recall.\n",
    "# #\n",
    "# # Example:\n",
    "# #   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "# #   In [2]: predictions = [0.3, 0.4, 0.6, 0.7, 0.8, 0.8]\n",
    "# #   In [3]: auroc, auprc = compute_auc(labels, predictions)\n",
    "# #   In [4]: auroc\n",
    "# #   Out[4]: 1.0\n",
    "# #   In [5]: auprc\n",
    "# #   Out[5]: 1.0\n",
    "\n",
    "# def compute_auc(labels, predictions, check_errors=True):\n",
    "#     # Check inputs for errors.\n",
    "#     #print('labels inside auc_fucn',labels)\n",
    "#     if check_errors:\n",
    "#         if len(predictions) != len(labels):\n",
    "#             raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "#         for label in labels:\n",
    "#             if not label in (0, 1):\n",
    "#                 raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "#         for prediction in predictions:\n",
    "#             if not 0 <= prediction <= 1:\n",
    "#                 warnings.warn('Predictions do not satisfy 0 <= prediction <= 1.')\n",
    "\n",
    "#     # Find prediction thresholds.\n",
    "#     thresholds = np.unique(predictions)[::-1]\n",
    "#     if thresholds[0] != 1:\n",
    "#         thresholds = np.insert(thresholds, 0, 1)\n",
    "#     if thresholds[-1] == 0:\n",
    "#         thresholds = thresholds[:-1]\n",
    "\n",
    "#     n = len(labels)\n",
    "#     m = len(thresholds)\n",
    "\n",
    "#     # Populate contingency table across prediction thresholds.\n",
    "#     tp = np.zeros(m)\n",
    "#     fp = np.zeros(m)\n",
    "#     fn = np.zeros(m)\n",
    "#     tn = np.zeros(m)\n",
    "\n",
    "#     # Find indices that sort the predicted probabilities from largest to\n",
    "#     # smallest.\n",
    "#     idx = np.argsort(predictions)[::-1]\n",
    "\n",
    "#     i = 0\n",
    "#     for j in range(m):\n",
    "#         # Initialize contingency table for j-th prediction threshold.\n",
    "#         if j == 0:\n",
    "#             tp[j] = 0\n",
    "#             fp[j] = 0\n",
    "#             fn[j] = np.sum(labels)\n",
    "#             tn[j] = n - fn[j]\n",
    "#         else:\n",
    "#             tp[j] = tp[j - 1]\n",
    "#             fp[j] = fp[j - 1]\n",
    "#             fn[j] = fn[j - 1]\n",
    "#             tn[j] = tn[j - 1]\n",
    "\n",
    "#         # Update contingency table for i-th largest predicted probability.\n",
    "#         while i < n and predictions[idx[i]] >= thresholds[j]:\n",
    "#             if labels[idx[i]]:\n",
    "#                 tp[j] += 1\n",
    "#                 fn[j] -= 1\n",
    "#             else:\n",
    "#                 fp[j] += 1\n",
    "#                 tn[j] -= 1\n",
    "#             i += 1\n",
    "\n",
    "#     # Summarize contingency table.\n",
    "#     tpr = np.zeros(m)\n",
    "#     tnr = np.zeros(m)\n",
    "#     ppv = np.zeros(m)\n",
    "#     npv = np.zeros(m)\n",
    "\n",
    "#     for j in range(m):\n",
    "#         if tp[j] + fn[j]:\n",
    "#             tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "#         else:\n",
    "#             tpr[j] = 1\n",
    "#         if fp[j] + tn[j]:\n",
    "#             tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "#         else:\n",
    "#             tnr[j] = 1\n",
    "#         if tp[j] + fp[j]:\n",
    "#             ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "#         else:\n",
    "#             ppv[j] = 1\n",
    "#         if fn[j] + tn[j]:\n",
    "#             npv[j] = tn[j] / (fn[j] + tn[j])\n",
    "#         else:\n",
    "#             npv[j] = 1\n",
    "\n",
    "#     # Compute AUROC as the area under a piecewise linear function with TPR /\n",
    "#     # sensitivity (x-axis) and TNR / specificity (y-axis) and AUPRC as the area\n",
    "#     # under a piecewise constant with TPR / recall (x-axis) and PPV / precision\n",
    "#     # (y-axis).\n",
    "#     auroc = 0\n",
    "#     auprc = 0\n",
    "#     for j in range(m-1):\n",
    "#         auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])\n",
    "#         auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]\n",
    "\n",
    "#     return auroc, auprc\n",
    "\n",
    "# # The compute_accuracy_f_measure function computes the accuracy and F-measure\n",
    "# # for a patient.\n",
    "# #\n",
    "# # Inputs:\n",
    "# #   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "# #   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "# #   septic at time i.\n",
    "# #\n",
    "# #   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "# #   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "# #   patient is predicted to be septic at time i.  Note that there must be a\n",
    "# #   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "# #\n",
    "# # Output:\n",
    "# #   'accuracy' is a scalar that gives the accuracy of the predictions using its\n",
    "# #   binarized predictions.\n",
    "# #\n",
    "# #   'f_measure' is a scalar that gives the F-measure of the predictions using its\n",
    "# #   binarized predictions.\n",
    "# #\n",
    "# # Example:\n",
    "# #   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "# #   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "# #   In [3]: accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "# #   In [4]: accuracy\n",
    "# #   Out[4]: 0.666666666667\n",
    "# #   In [5]: f_measure\n",
    "# #   Out[5]: 0.666666666667\n",
    "\n",
    "# def compute_accuracy_f_measure(labels, predictions, check_errors=True):\n",
    "#     # Check inputs for errors.\n",
    "#     if check_errors:\n",
    "#         if len(predictions) != len(labels):\n",
    "#             raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "#         for label in labels:\n",
    "#             if not label in (0, 1):\n",
    "#                 raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "#         for prediction in predictions:\n",
    "#             if not prediction in (0, 1):\n",
    "#                 raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "#     # Populate contingency table.\n",
    "#     n = len(labels)\n",
    "#     tp = 0\n",
    "#     fp = 0\n",
    "#     fn = 0\n",
    "#     tn = 0\n",
    "\n",
    "#     for i in range(n):\n",
    "#         if labels[i] and predictions[i]:\n",
    "#             tp += 1\n",
    "#         elif not labels[i] and predictions[i]:\n",
    "#             fp += 1\n",
    "#         elif labels[i] and not predictions[i]:\n",
    "#             fn += 1\n",
    "#         elif not labels[i] and not predictions[i]:\n",
    "#             tn += 1\n",
    "\n",
    "#     # Summarize contingency table.\n",
    "#     if tp + fp + fn + tn:\n",
    "#         accuracy = float(tp + tn) / float(tp + fp + fn + tn)\n",
    "#     else:\n",
    "#         accuracy = 1.0\n",
    "\n",
    "#     if 2 * tp + fp + fn:\n",
    "#         f_measure = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "#     else:\n",
    "#         f_measure = 1.0\n",
    "\n",
    "#     return accuracy, f_measure\n",
    "\n",
    "# # The compute_prediction_utility function computes the total time-dependent\n",
    "# # utility for a patient.\n",
    "# #\n",
    "# # Inputs:\n",
    "# #   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "# #   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "# #   septic at time i.\n",
    "# #\n",
    "# #   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "# #   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "# #   patient is predicted to be septic at time i.  Note that there must be a\n",
    "# #   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "# #\n",
    "# # Output:\n",
    "# #   'utility' is a scalar that gives the total time-dependent utility of the\n",
    "# #   algorithm using its binarized predictions.\n",
    "# #\n",
    "# # Example:\n",
    "# #   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "# #   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "# #   In [3]: utility = compute_prediction_utility(labels, predictions)\n",
    "# #   In [4]: utility\n",
    "# #   Out[4]: 3.388888888888889\n",
    "\n",
    "# def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0, check_errors=True):\n",
    "#     # Check inputs for errors.\n",
    "#     if check_errors:\n",
    "#         if len(predictions) != len(labels):\n",
    "#             raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "#         for label in labels:\n",
    "#             if not label in (0, 1):\n",
    "#                 raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "#         for prediction in predictions:\n",
    "#             if not prediction in (0, 1):\n",
    "#                 raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "#         if dt_early >= dt_optimal:\n",
    "#             raise Exception('The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "#         if dt_optimal >= dt_late:\n",
    "#             raise Exception('The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "#     # Does the patient eventually have sepsis?\n",
    "#     if np.any(labels):\n",
    "#         is_septic = True\n",
    "#         t_sepsis = np.argmax(labels) - dt_optimal\n",
    "#     else:\n",
    "#         is_septic = False\n",
    "#         t_sepsis = float('inf')\n",
    "\n",
    "#     n = len(labels)\n",
    "\n",
    "#     # Define slopes and intercept points for utility functions of the form\n",
    "#     # u = m * t + b.\n",
    "#     m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "#     b_1 = -m_1 * dt_early\n",
    "#     m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "#     b_2 = -m_2 * dt_late\n",
    "#     m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "#     b_3 = -m_3 * dt_optimal\n",
    "\n",
    "#     # Compare predicted and true conditions.\n",
    "#     u = np.zeros(n)\n",
    "#     for t in range(n):\n",
    "#         if t <= t_sepsis + dt_late:\n",
    "#             # TP\n",
    "#             if is_septic and predictions[t]:\n",
    "#                 if t <= t_sepsis + dt_optimal:\n",
    "#                     u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "#                 elif t <= t_sepsis + dt_late:\n",
    "#                     u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "#             # FP\n",
    "#             elif not is_septic and predictions[t]:\n",
    "#                 u[t] = u_fp\n",
    "#             # FN\n",
    "#             elif is_septic and not predictions[t]:\n",
    "#                 if t <= t_sepsis + dt_optimal:\n",
    "#                     u[t] = 0\n",
    "#                 elif t <= t_sepsis + dt_late:\n",
    "#                     u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "#             # TN\n",
    "#             elif not is_septic and not predictions[t]:\n",
    "#                 u[t] = u_tn\n",
    "\n",
    "#     # Find total utility for patient.\n",
    "#     return np.sum(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
